{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/4.topics/TopicModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR3AOAE100Oy"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/4.topics/TopicModel.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9IMbBbn2vK1"
      },
      "source": [
        "# Topic modeling movie summaries\n",
        "\n",
        "In this notebook we'll use topic modeling to discover broad themes in a collection of movie summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S6aN-6y00O3",
        "outputId": "a7b61041-5f0a-443e-985d-791d91c02211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiJYciPG2vK4",
        "outputId": "212034d7-a154-41a8-8aee-9bb15c7ec900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from tqdm import tqdm  # for progress bars\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZF3Si83290-",
        "outputId": "d93ab9ba-b255-4139-bba6-0b4b3c661dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-16 23:23:45--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/jockers.stopwords\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44453 (43K) [text/plain]\n",
            "Saving to: ‘jockers.stopwords’\n",
            "\n",
            "\rjockers.stopwords     0%[                    ]       0  --.-KB/s               \rjockers.stopwords   100%[===================>]  43.41K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-09-16 23:23:46 (5.29 MB/s) - ‘jockers.stopwords’ saved [44453/44453]\n",
            "\n",
            "--2025-09-16 23:23:46--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/movie.metadata.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15038604 (14M) [text/plain]\n",
            "Saving to: ‘movie.metadata.tsv’\n",
            "\n",
            "movie.metadata.tsv  100%[===================>]  14.34M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-09-16 23:23:46 (160 MB/s) - ‘movie.metadata.tsv’ saved [15038604/15038604]\n",
            "\n",
            "--2025-09-16 23:23:46--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/plot_summaries.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75934033 (72M) [text/plain]\n",
            "Saving to: ‘plot_summaries.txt’\n",
            "\n",
            "plot_summaries.txt  100%[===================>]  72.42M   250MB/s    in 0.3s    \n",
            "\n",
            "2025-09-16 23:23:46 (250 MB/s) - ‘plot_summaries.txt’ saved [75934033/75934033]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/main/data/jockers.stopwords\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/main/data/movie.metadata.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/main/data/plot_summaries.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1phZTyi2vK6"
      },
      "source": [
        "## Loading stopwords\n",
        "\n",
        "Since we're running topic modeling on texts with lots of names, we'll add the Jockers list of stopwords (which includes character names) to our stoplist. We'll also filter out any words that don't contain at least one letter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BWKhujgi2vK6"
      },
      "outputs": [],
      "source": [
        "def read_stopwords(filename):\n",
        "    \"\"\"Reads a file of stopwords into a set.\"\"\"\n",
        "    stopwords = set([\n",
        "        line.rstrip() for line in open(filename)\n",
        "    ])\n",
        "    return stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H8-0-X_-2vK7"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words = stop_words | read_stopwords(\"jockers.stopwords\")\n",
        "stop_words.add(\"'s\")\n",
        "stop_words=list(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iAMInj_R2vK7"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r\"[A-Za-z]\")\n",
        "def stopword_filter(word, stopwords):\n",
        "    \"\"\" Function to exclude words from a text.\"\"\"\n",
        "\n",
        "    # no stopwords\n",
        "    if word in stopwords:\n",
        "        return False\n",
        "\n",
        "    # has to contain at least one letter\n",
        "    if pattern.search(word) is not None:\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AF6QANy2vK8"
      },
      "source": [
        "## Loading summaries\n",
        "\n",
        "We'll read in summaries of the 5,000 movies with the highest box office revenues. This may take 3-4 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wIxxxBWu2vK7"
      },
      "outputs": [],
      "source": [
        "def read_docs(plot_filename, metadata_filename, stopwords):\n",
        "    n=5000\n",
        "\n",
        "    # only get box office top N\n",
        "    metadata = pd.read_csv(metadata_filename, sep=\"\\t\", names=[\"movie_id\", \"_\", \"title\", \"year\", \"box_office\", \"_1\", \"_2\", \"_3\", \"_4\"])\n",
        "    metadata = metadata.dropna(subset=[\"box_office\"]).sort_values(by=\"box_office\", ascending=False)\n",
        "    metadata = metadata.iloc[:n].set_index(\"movie_id\")\n",
        "\n",
        "    plots = pd.read_csv(plot_filename, sep=\"\\t\", names=[\"movie_id\", \"summary\"])\n",
        "    plots = plots.set_index(\"movie_id\")\n",
        "    plots = metadata.join(plots)\n",
        "\n",
        "    def tokenize_and_process(text):\n",
        "        return [\n",
        "            x for x in nltk.word_tokenize(text.lower()) if stopword_filter(x, stopwords)\n",
        "        ]\n",
        "\n",
        "    docs = []\n",
        "    for summary in tqdm(plots.summary.fillna(\"\")):\n",
        "        docs.append(tokenize_and_process(summary))\n",
        "\n",
        "    names = plots.title.to_list()\n",
        "    return docs, list(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XaYqSs8F2vK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c286ec9-2d0d-49b0-dfd4-eaf6e7e82cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [03:53<00:00, 21.40it/s]\n"
          ]
        }
      ],
      "source": [
        "docs, names = read_docs(\"plot_summaries.txt\", \"movie.metadata.tsv\", stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQUWExT22vK9"
      },
      "source": [
        "We will convert the movie summaries into a bag-of-words representation using gensim's [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R0zxKTmJ2vK9"
      },
      "outputs": [],
      "source": [
        "# Create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents\n",
        "# and no more than 50% of all documents\n",
        "\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tAAsrxaI2vK9"
      },
      "outputs": [],
      "source": [
        "# Replace dataset with numeric ids words in vocab (and exclude all other words)\n",
        "corpus = [dictionary.doc2bow(text) for text in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJqjhoFL2vK-"
      },
      "source": [
        "## Running topic model\n",
        "\n",
        "Now let's run a topic model on this data using gensim's built-in LDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4hE64x_22vK9"
      },
      "outputs": [],
      "source": [
        "num_topics = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "373pP7-02vK-"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    passes=10,\n",
        "    alpha='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUduxAS92vK-"
      },
      "source": [
        "## Interpreting topic model\n",
        "\n",
        "**Topic word distributions**\n",
        "\n",
        "We can get a sense of what the topics are by printing the top 10 words with highest $P(word \\mid topic)$ for each topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG3HiUOC2vK-",
        "outputId": "a468452f-05b8-4f70-c768-3bbef3806268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic 0:\tfight kid stone tournament match martial boxing training arts evil\n",
            "topic 1:\tearth alien dr. planet space ship machine aliens human humans\n",
            "topic 2:\tbond collins dracula monster gibson count frankenstein phantom coffin bride\n",
            "topic 3:\tfather village camp death wolf de lord killed family men\n",
            "topic 4:\trace murphy vegas batman dodge thompson pink las stark casino\n",
            "topic 5:\tsmith president wheeler murder political film company agent death government\n",
            "topic 6:\tteam game school film students high win play coach time\n",
            "topic 7:\tpolice hotel wife train prison apartment murder time takes tells\n",
            "topic 8:\tbeck miller stu silver sullivan snake vic bush paulie tooth\n",
            "topic 9:\tship crew island boat water captain escape find group sea\n",
            "topic 10:\tfather tells mother family life day time house school finds\n",
            "topic 11:\twar army soldiers men general attack orders colonel military battle\n",
            "topic 12:\tfind world city castle life help return named finds children\n",
            "topic 13:\tghost christmas spirit mill doll oz frost fairies malone dolls\n",
            "topic 14:\thouse body find finds room killed kill car police dead\n",
            "topic 15:\tagent team escape bomb kill killed plane fbi kills agents\n",
            "topic 16:\ttown horse farm sheriff bishop men west warden prison land\n",
            "topic 17:\tpolice money car gang men gun bank shoots gets kill\n",
            "topic 18:\tfilm show band music stage club dance big rock studio\n",
            "topic 19:\tbook story novel malloy connor writer reading werewolf ghost author\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_topics):\n",
        "    print(\"topic %s:\\t%s\" % (i, ' '.join([term for term, freq in lda_model.show_topic(i, topn=10)])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yirq9huh2vK-"
      },
      "source": [
        "**Topic document distributions**\n",
        "\n",
        "Another way of understanding topics is to print out the documents that have the highest topic representation -- i.e., for a given topic $k$, the documents with highest $P(\\text{topic}=k \\mid \\text{document})$.  How much do the documents listed here align with your understanding of the topics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDrsC_zz2vK-",
        "outputId": "1cf34c79-5113-4cbc-ab9f-3dd07c337f65",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fight kid stone tournament match martial boxing training arts evil\n",
            "\n",
            "0\t0.712681233882904\tSidekicks\n",
            "0\t0.7078146934509277\t3 Ninjas Kick Back\n",
            "0\t0.6681515574455261\tBloodsport\n",
            "0\t0.5996115803718567\tThe Meteor Man\n",
            "0\t0.5739623308181763\tSalsa\n",
            "\n",
            "earth alien dr. planet space ship machine aliens human humans\n",
            "\n",
            "1\t0.9143122434616089\tQueen of Blood\n",
            "1\t0.6407159566879272\tStar Trek: The Motion Picture\n",
            "1\t0.6007406115531921\tMoon\n",
            "1\t0.589035153388977\tAlien: Resurrection\n",
            "1\t0.552208423614502\tBattle for Terra\n",
            "\n",
            "bond collins dracula monster gibson count frankenstein phantom coffin bride\n",
            "\n",
            "2\t0.6625617146492004\tDracula: Dead and Loving It\n",
            "2\t0.4098891019821167\tPrivate Eye\n",
            "2\t0.35834944248199463\tDracula 2000\n",
            "2\t0.35705670714378357\tThe Man with the Golden Gun\n",
            "2\t0.31080031394958496\tTerror Train\n",
            "\n",
            "father village camp death wolf de lord killed family men\n",
            "\n",
            "3\t0.9747889637947083\tJust Heroes\n",
            "3\t0.936826765537262\tWhite Fang 2: Myth of the White Wolf\n",
            "3\t0.9367643594741821\tThe Ottoman Republic\n",
            "3\t0.9210546016693115\tShinobi: Heart Under Blade\n",
            "3\t0.7530941963195801\tAmalfi: Rewards of the Goddess\n",
            "\n",
            "race murphy vegas batman dodge thompson pink las stark casino\n",
            "\n",
            "4\t0.5101520419120789\tThe High and the Mighty\n",
            "4\t0.5095282196998596\tWhere the Buffalo Roam\n",
            "4\t0.49779289960861206\tTrail of the Pink Panther\n",
            "4\t0.4395506680011749\tMurphy's Law\n",
            "4\t0.41512057185173035\tHerbie: Fully Loaded\n",
            "\n",
            "smith president wheeler murder political film company agent death government\n",
            "\n",
            "5\t0.9558149576187134\tThe Two Jakes\n",
            "5\t0.9106426239013672\tV.I. Warshawski\n",
            "5\t0.8819997310638428\tRed Corner\n",
            "5\t0.8607296347618103\tBloodline\n",
            "5\t0.8269578814506531\tFair Game\n",
            "\n",
            "team game school film students high win play coach time\n",
            "\n",
            "6\t0.9836826920509338\tThe Express\n",
            "6\t0.9790248274803162\tHoop Dreams\n",
            "6\t0.9554800987243652\tThe September Issue\n",
            "6\t0.948167622089386\tMystery, Alaska\n",
            "6\t0.9473843574523926\tSarafina!\n",
            "\n",
            "police hotel wife train prison apartment murder time takes tells\n",
            "\n",
            "7\t0.9853431582450867\tGigli\n",
            "7\t0.9739024639129639\tFor Love or Money\n",
            "7\t0.9239591360092163\tUlee's Gold\n",
            "7\t0.9230836629867554\tThe Temp\n",
            "7\t0.9221326112747192\tRush\n",
            "\n",
            "beck miller stu silver sullivan snake vic bush paulie tooth\n",
            "\n",
            "8\t0.44982030987739563\tArticle 99\n",
            "8\t0.4033627212047577\tBats\n",
            "8\t0.24124333262443542\tHeartbeat\n",
            "8\t0.2073352038860321\tW.\n",
            "8\t0.20585064589977264\tThe Hidden\n",
            "\n",
            "ship crew island boat water captain escape find group sea\n",
            "\n",
            "9\t0.8392795920372009\tDeepstar Six\n",
            "9\t0.7780925631523132\tDeep Rising\n",
            "9\t0.7443163394927979\tThe Abyss\n",
            "9\t0.7311646342277527\tSpeed 2: Cruise Control\n",
            "9\t0.6631019115447998\tJaws 3-D\n",
            "\n",
            "father tells mother family life day time house school finds\n",
            "\n",
            "10\t0.996282160282135\tGood Deeds\n",
            "10\t0.9945217370986938\tWhen Harry Met Sally...\n",
            "10\t0.9939175844192505\tGeorgy Girl\n",
            "10\t0.9927675127983093\tAnother Year\n",
            "10\t0.9925291538238525\tBaby Mama\n",
            "\n",
            "war army soldiers men general attack orders colonel military battle\n",
            "\n",
            "11\t0.9480222463607788\tGettysburg\n",
            "11\t0.9366788268089294\tThe Big Red One\n",
            "11\t0.8946758508682251\tR-Point\n",
            "11\t0.8629093170166016\tWe Were Soldiers\n",
            "11\t0.8354814648628235\tThe Steel Helmet\n",
            "\n",
            "find world city castle life help return named finds children\n",
            "\n",
            "12\t0.9913273453712463\tRed Sonja\n",
            "12\t0.9860275387763977\tKull the Conqueror\n",
            "12\t0.9454432725906372\tThe Lord of the Rings\n",
            "12\t0.9329093098640442\tThe Black Cauldron\n",
            "12\t0.925620973110199\tConan the Destroyer\n",
            "\n",
            "ghost christmas spirit mill doll oz frost fairies malone dolls\n",
            "\n",
            "13\t0.8621997833251953\tThe General\n",
            "13\t0.6852641105651855\tGhost Dog: The Way of the Samurai\n",
            "13\t0.2781992554664612\tIndiana Jones and the Raiders of the Lost Ark\n",
            "13\t0.2322360724210739\tThe Flowers of War\n",
            "13\t0.2132105529308319\tTinker Bell and the Great Fairy Rescue\n",
            "\n",
            "house body find finds room killed kill car police dead\n",
            "\n",
            "14\t0.9706547260284424\tHalloween 5: The Revenge of Michael Myers\n",
            "14\t0.9511316418647766\tThe House on Sorority Row\n",
            "14\t0.940861165523529\tThe Beast Within\n",
            "14\t0.9371552467346191\tSaw\n",
            "14\t0.9364154934883118\tHalloween\n",
            "\n",
            "agent team escape bomb kill killed plane fbi kills agents\n",
            "\n",
            "15\t0.9461033940315247\tThe Ambushers\n",
            "15\t0.9232547283172607\tThe Amateur\n",
            "15\t0.8761171102523804\tPassenger 57\n",
            "15\t0.8669160008430481\tTurbulence\n",
            "15\t0.8153066039085388\tCity of Damnation\n",
            "\n",
            "town horse farm sheriff bishop men west warden prison land\n",
            "\n",
            "16\t0.9768450856208801\tBig Stan\n",
            "16\t0.6413375735282898\tThe Wild Bunch\n",
            "16\t0.5158464908599854\tBandolero!\n",
            "16\t0.4470842182636261\tStar Kid\n",
            "16\t0.4453897774219513\tA Thousand Acres\n",
            "\n",
            "police money car gang men gun bank shoots gets kill\n",
            "\n",
            "17\t0.8793396949768066\tTrapped in Paradise\n",
            "17\t0.8648117780685425\tBig Jake\n",
            "17\t0.8473332524299622\tBlack Dog\n",
            "17\t0.8455237150192261\tDrive\n",
            "17\t0.8095911741256714\tThunderbolt and Lightfoot\n",
            "\n",
            "film show band music stage club dance big rock studio\n",
            "\n",
            "18\t0.9026368260383606\tCadillac Records\n",
            "18\t0.888287365436554\tStill Smokin'\n",
            "18\t0.8265925049781799\tConnie and Carla\n",
            "18\t0.8213397264480591\tSingin' in the Rain\n",
            "18\t0.8026860356330872\tDreamgirls\n",
            "\n",
            "book story novel malloy connor writer reading werewolf ghost author\n",
            "\n",
            "19\t0.8470401763916016\tBeverly Hills Chihuahua 3\n",
            "19\t0.6401954293251038\tLe Grand Chef\n",
            "19\t0.6259778738021851\tAsylum\n",
            "19\t0.4660310745239258\tPossession\n",
            "19\t0.4142228960990906\tBoogeyman 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "topic_model = lda_model\n",
        "topic_docs = []\n",
        "\n",
        "for i in range(num_topics):\n",
        "    topic_docs.append({})\n",
        "\n",
        "for doc_id in range(len(corpus)):\n",
        "    doc_topics = topic_model.get_document_topics(corpus[doc_id])\n",
        "    for topic_num, topic_prob in doc_topics:\n",
        "        topic_docs[topic_num][doc_id] = topic_prob\n",
        "\n",
        "for i in range(num_topics):\n",
        "    top_topic_terms = [term for term, _ in topic_model.show_topic(i, topn=10)]\n",
        "    sorted_docs = sorted(topic_docs[i].items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\" \".join(top_topic_terms))\n",
        "    print()\n",
        "\n",
        "    for doc_id, prob in sorted_docs[:5]:\n",
        "        print(f\"{i}\\t{prob}\\t{names[doc_id]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results seem generally consistent at pairing films with their respective topic. For documents that I am familiar with, like Star Trek, it's assignment to the topic with 'alien', 'ships', 'planet,' etc. feels fitting. I expect that this topic representation breaks down a bit for documents containing words that fall under multiple distinct topics. This list notes movies by top P(topic=k ∣ document) values, which indicates movies that are extremely representative of their allocated topic, and not much else, because having a large proportion of the document being about another topic would dilute the probability."
      ],
      "metadata": {
        "id": "iqw0HzeC5Pue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps1k2yMu2vK-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}