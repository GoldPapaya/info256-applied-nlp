{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/4.topics/TopicModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR3AOAE100Oy"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/4.topics/TopicModel.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9IMbBbn2vK1"
      },
      "source": [
        "# Topic modeling movie summaries\n",
        "\n",
        "In this notebook we'll use topic modeling to discover broad themes in a collection of movie summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3S6aN-6y00O3",
        "outputId": "3ab3f2d9-c93a-4e54-c67d-f587419ac71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "cf468dd87794462fad9a017163a06649"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiJYciPG2vK4"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from tqdm import tqdm  # for progress bars\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZF3Si83290-"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/main/data/jockers.stopwords\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/main/data/movie.metadata.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/main/data/plot_summaries.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1phZTyi2vK6"
      },
      "source": [
        "## Loading stopwords\n",
        "\n",
        "Since we're running topic modeling on texts with lots of names, we'll add the Jockers list of stopwords (which includes character names) to our stoplist. We'll also filter out any words that don't contain at least one letter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWKhujgi2vK6"
      },
      "outputs": [],
      "source": [
        "def read_stopwords(filename):\n",
        "    \"\"\"Reads a file of stopwords into a set.\"\"\"\n",
        "    stopwords = set([\n",
        "        line.rstrip() for line in open(filename)\n",
        "    ])\n",
        "    return stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8-0-X_-2vK7"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words = stop_words | read_stopwords(\"jockers.stopwords\")\n",
        "stop_words.add(\"'s\")\n",
        "stop_words=list(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAMInj_R2vK7"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r\"[A-Za-z]\")\n",
        "def stopword_filter(word, stopwords):\n",
        "    \"\"\" Function to exclude words from a text.\"\"\"\n",
        "\n",
        "    # no stopwords\n",
        "    if word in stopwords:\n",
        "        return False\n",
        "\n",
        "    # has to contain at least one letter\n",
        "    if pattern.search(word) is not None:\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AF6QANy2vK8"
      },
      "source": [
        "## Loading summaries\n",
        "\n",
        "We'll read in summaries of the 5,000 movies with the highest box office revenues. This may take 3-4 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIxxxBWu2vK7"
      },
      "outputs": [],
      "source": [
        "def read_docs(plot_filename, metadata_filename, stopwords):\n",
        "    n=5000\n",
        "\n",
        "    # only get box office top N\n",
        "    metadata = pd.read_csv(metadata_filename, sep=\"\\t\", names=[\"movie_id\", \"_\", \"title\", \"year\", \"box_office\", \"_1\", \"_2\", \"_3\", \"_4\"])\n",
        "    metadata = metadata.dropna(subset=[\"box_office\"]).sort_values(by=\"box_office\", ascending=False)\n",
        "    metadata = metadata.iloc[:n].set_index(\"movie_id\")\n",
        "\n",
        "    plots = pd.read_csv(plot_filename, sep=\"\\t\", names=[\"movie_id\", \"summary\"])\n",
        "    plots = plots.set_index(\"movie_id\")\n",
        "    plots = metadata.join(plots)\n",
        "\n",
        "    def tokenize_and_process(text):\n",
        "        return [\n",
        "            x for x in nltk.word_tokenize(text.lower()) if stopword_filter(x, stopwords)\n",
        "        ]\n",
        "\n",
        "    docs = []\n",
        "    for summary in tqdm(plots.summary.fillna(\"\")):\n",
        "        docs.append(tokenize_and_process(summary))\n",
        "\n",
        "    names = plots.title.to_list()\n",
        "    return docs, list(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaYqSs8F2vK8"
      },
      "outputs": [],
      "source": [
        "docs, names = read_docs(\"plot_summaries.txt\", \"movie.metadata.tsv\", stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQUWExT22vK9"
      },
      "source": [
        "We will convert the movie summaries into a bag-of-words representation using gensim's [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0zxKTmJ2vK9"
      },
      "outputs": [],
      "source": [
        "# Create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents\n",
        "# and no more than 50% of all documents\n",
        "\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAAsrxaI2vK9"
      },
      "outputs": [],
      "source": [
        "# Replace dataset with numeric ids words in vocab (and exclude all other words)\n",
        "corpus = [dictionary.doc2bow(text) for text in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJqjhoFL2vK-"
      },
      "source": [
        "## Running topic model\n",
        "\n",
        "Now let's run a topic model on this data using gensim's built-in LDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hE64x_22vK9"
      },
      "outputs": [],
      "source": [
        "num_topics = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "373pP7-02vK-"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    passes=10,\n",
        "    alpha='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUduxAS92vK-"
      },
      "source": [
        "## Interpreting topic model\n",
        "\n",
        "**Topic word distributions**\n",
        "\n",
        "We can get a sense of what the topics are by printing the top 10 words with highest $P(word \\mid topic)$ for each topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG3HiUOC2vK-"
      },
      "outputs": [],
      "source": [
        "for i in range(num_topics):\n",
        "    print(\"topic %s:\\t%s\" % (i, ' '.join([term for term, freq in lda_model.show_topic(i, topn=10)])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yirq9huh2vK-"
      },
      "source": [
        "**Topic document distributions**\n",
        "\n",
        "Another way of understanding topics is to print out the documents that have the highest topic representation -- i.e., for a given topic $k$, the documents with highest $P(\\text{topic}=k \\mid \\text{document})$.  How much do the documents listed here align with your understanding of the topics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDrsC_zz2vK-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "topic_model = lda_model\n",
        "topic_docs = []\n",
        "\n",
        "for i in range(num_topics):\n",
        "    topic_docs.append({})\n",
        "\n",
        "for doc_id in range(len(corpus)):\n",
        "    doc_topics = topic_model.get_document_topics(corpus[doc_id])\n",
        "    for topic_num, topic_prob in doc_topics:\n",
        "        topic_docs[topic_num][doc_id] = topic_prob\n",
        "\n",
        "for i in range(num_topics):\n",
        "    top_topic_terms = [term for term, _ in topic_model.show_topic(i, topn=10)]\n",
        "    sorted_docs = sorted(topic_docs[i].items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\" \".join(top_topic_terms))\n",
        "    print()\n",
        "\n",
        "    for doc_id, prob in sorted_docs[:5]:\n",
        "        print(f\"{i}\\t{prob}\\t{names[doc_id]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps1k2yMu2vK-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}