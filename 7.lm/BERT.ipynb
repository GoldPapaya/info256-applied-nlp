{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/7.lm/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5792cb96",
      "metadata": {
        "id": "5792cb96"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/7.lm/BERT.ipynb)\n",
        "\n",
        "In this notebook, we'll explore the basics of token representations in BERT and use it to find token nearest neighbors.  You should open this notebook in Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eddc388c",
      "metadata": {
        "id": "eddc388c"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "88a12638",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a12638",
        "outputId": "79090889-4e45-49c5-9767-599ef2a92ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f34541",
      "metadata": {
        "id": "82f34541"
      },
      "source": [
        "BERT uses WordPiece tokenization, which breaks down words that don't appear within its 30K-word vocabulary into small pieces.  The word \"Ishmael\", for instanced, is tokenized as [\"is\", \"##hma\", \"##el\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b2a1e451",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2a1e451",
        "outputId": "91df5d3f-13cc-4c89-ea72-2c6fa48fc724"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'call', 'me', 'is', '##hma', '##el', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "inputs = tokenizer(\"Call me Ishmael\", return_tensors=\"pt\")\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a390bb2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a390bb2d",
        "outputId": "27d695e3-5128-40d2-fabc-76527177b0fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'bert',\n",
              " 'is',\n",
              " 'super',\n",
              " '##cal',\n",
              " '##if',\n",
              " '##rag',\n",
              " '##ilis',\n",
              " '##tic',\n",
              " '##ex',\n",
              " '##pia',\n",
              " '##lid',\n",
              " '##oc',\n",
              " '##ious',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "inputs = tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b058b2be",
      "metadata": {
        "id": "b058b2be"
      },
      "source": [
        "As mentioned in class, notice how every sentence input to BERT is wrapped in two tags: a start [CLS] tag and an ending [SEP] tag.  BERT will generate representations of each WordPiece token, including these special [CLS] and [SEP] tags."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a9c2fa",
      "metadata": {
        "id": "e4a9c2fa"
      },
      "source": [
        "To generate representations for the input tokens, simply pass the input through the BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e2bc73aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bc73aa",
        "outputId": "a29d2cb9-e446-493c-807a-ab3fddee706a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'this', 'jam', 'is', 'delicious', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "inputs = tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe3f309",
      "metadata": {
        "id": "bbe3f309"
      },
      "source": [
        "Representations for each of BERT layers (12 in this model) are accessible here, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input ([CLS, this, jam, is, delicious, [SEP]) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "967d8d59",
      "metadata": {
        "id": "967d8d59"
      },
      "outputs": [],
      "source": [
        "last_hidden_states = outputs.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6fe68cc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fe68cc0",
        "outputId": "5a4725b3-5121-4bbe-f46f-795b48e4b633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 768])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c888e87d",
      "metadata": {
        "id": "c888e87d"
      },
      "source": [
        "What can we do with just these representations?  While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6eccbb32",
      "metadata": {
        "id": "6eccbb32"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b79b4ed",
      "metadata": {
        "id": "0b79b4ed"
      },
      "outputs": [],
      "source": [
        "query = \"I ate some jam with toast\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cbc6477e",
      "metadata": {
        "id": "cbc6477e"
      },
      "outputs": [],
      "source": [
        "comp_sents = [\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "be898d38",
      "metadata": {
        "id": "be898d38"
      },
      "outputs": [],
      "source": [
        "def get_bert_for_token(string, term):\n",
        "\n",
        "    # tokenize\n",
        "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
        "\n",
        "    # convert input ids to words\n",
        "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
        "    term_idx=tokens.index(term)\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # return the BERT rep for that token index\n",
        "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
        "\n",
        "    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "719345c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "719345c7",
        "outputId": "6f58d865-2f32-4266-c3cf-15afb5914f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768,)\n"
          ]
        }
      ],
      "source": [
        "query_rep=get_bert_for_token(query, \"jam\")\n",
        "print(query_rep.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ddd0e1e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd0e1e8",
        "outputId": "0d3d43a1-ccec-4c72-ada6-efaa8081d943",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.736\tI ate some jam with toast\tMy grandma makes the best jam.\n",
            "0.665\tI ate some jam with toast\tThis jam is made of strawberries\n",
            "0.480\tI ate some jam with toast\tI sat in a traffic jam for 2 hours\n",
            "0.443\tI ate some jam with toast\tThe Grateful Dead used to jam for like two days straight.\n",
            "0.385\tI ate some jam with toast\tShe got me out of a real jam\n",
            "0.290\tI ate some jam with toast\tI had to jam on the brakes to avoid hitting him.\n"
          ]
        }
      ],
      "source": [
        "vals=[]\n",
        "for sent in comp_sents:\n",
        "    comp_rep = get_bert_for_token(sent, \"jam\")\n",
        "    cos_sim = cosine_similarity(query_rep, comp_rep)\n",
        "    vals.append((cos_sim, query, sent))\n",
        "\n",
        "for c, q, s in reversed(sorted(vals)):\n",
        "    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7167cc",
      "metadata": {
        "id": "af7167cc"
      },
      "source": [
        "**Q**: Brainstorm the variety of things you can do with token-level word representations like this.  As one example, adapt the code above to find *any* word that is most similar to *jam* in \"I ate some jam with toast\" in the following sentences.  Are you able to find token-level synonyms this way?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9294232d",
      "metadata": {
        "id": "9294232d"
      },
      "outputs": [],
      "source": [
        "comp_sents = [\"My grandma makes the best jelly.\", \"Jelly is made of strawberries\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "42395734",
      "metadata": {
        "id": "42395734"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}