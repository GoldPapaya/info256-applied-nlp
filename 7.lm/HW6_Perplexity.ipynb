{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/7.lm/HW6_Perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45605ec-3f07-40d8-ab31-10a38ce83b79",
      "metadata": {
        "id": "d45605ec-3f07-40d8-ab31-10a38ce83b79"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/7.lm/HW6_Perplexity.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397af444-94c2-477b-92b8-4467b5ea80af",
      "metadata": {
        "id": "397af444-94c2-477b-92b8-4467b5ea80af"
      },
      "source": [
        "# HW6: Perplexity\n",
        "\n",
        "In this homework, you will implement a function to calculate the perplexity of the n-gram language models we covered in lab, and experiment with different sequences to better understand both n-gram LMs as well as the perplexity metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b52b3a60",
      "metadata": {
        "id": "b52b3a60",
        "outputId": "bf24c182-0307-46d4-c38d-3af7aa5dfded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import copy\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "33eb376a",
      "metadata": {
        "id": "33eb376a"
      },
      "outputs": [],
      "source": [
        "def read_file(filename):\n",
        "    sequences = []\n",
        "    with open(filename) as file:\n",
        "        data = file.read()\n",
        "        sents = sent_tokenize(data)\n",
        "        for sent in sents:\n",
        "            tokens = word_tokenize(sent)\n",
        "            sequences.append(tokens)\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a66e06bf-948f-4323-9af8-5864b466b033",
      "metadata": {
        "id": "a66e06bf-948f-4323-9af8-5864b466b033",
        "outputId": "0c398ca5-6ad8-4508-8ff4-f1a20639d4f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-03 18:03:48--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/1342_pride_and_prejudice.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 691804 (676K) [text/plain]\n",
            "Saving to: ‘1342_pride_and_prejudice.txt.1’\n",
            "\n",
            "\r          1342_prid   0%[                    ]       0  --.-KB/s               \r1342_pride_and_prej 100%[===================>] 675.59K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-10-03 18:03:48 (20.4 MB/s) - ‘1342_pride_and_prejudice.txt.1’ saved [691804/691804]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/1342_pride_and_prejudice.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "feeb46f4",
      "metadata": {
        "id": "feeb46f4"
      },
      "outputs": [],
      "source": [
        "# Read data from file and tokenize them into sequences comprised of tokens.\n",
        "\n",
        "# Pride and Prejudice (Jane Austen)\n",
        "sequences = read_file(\"1342_pride_and_prejudice.txt\")\n",
        "\n",
        "max_sequences = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "214dbd04",
      "metadata": {
        "id": "214dbd04"
      },
      "outputs": [],
      "source": [
        "class NgramModel():\n",
        "\n",
        "    def __init__(self, sequences, order):\n",
        "\n",
        "        # For this exercise we're going to encode the LM as a sparse dictionary (trading less storage for more compute)\n",
        "        # We'll store the LM as a dictionary with the conditioning context as keys; each value is a\n",
        "        # Counter object that keeps track of the number of times we see a word following that context.\n",
        "        self.counts = {}\n",
        "\n",
        "        # Markov order (order 1 = conditioning on previous 1 word; order 2 = previous 2 words, etc.)\n",
        "        self.order = order\n",
        "\n",
        "        vocab = {\"[END]\": 0}\n",
        "\n",
        "        for s_idx, tokens in enumerate(sequences):\n",
        "            # We'll add [START] and [END] tokens to encode the beginning/end of sentences\n",
        "            tokens = [\"[START]\"] * order + tokens + [\"[END]\"]\n",
        "\n",
        "            if s_idx == 0:\n",
        "                print(tokens)\n",
        "\n",
        "            for i in range(order, len(tokens)):\n",
        "                context = \" \".join(tokens[i - order:i])\n",
        "                word = tokens[i]\n",
        "\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = len(vocab)\n",
        "\n",
        "                # For just the first sentence, print the conditioning context + word\n",
        "                if s_idx == 0:\n",
        "                    print(\"Context: %s Next: %s\" % (context.ljust(50), word))\n",
        "\n",
        "                if context not in self.counts:\n",
        "                    self.counts[context] = Counter()\n",
        "                self.counts[context][word] += 1\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, context):\n",
        "        total = sum(self.counts[context].values())\n",
        "\n",
        "        dist = []\n",
        "        vocab = []\n",
        "\n",
        "        # Create a probability distribution for each conditioning context, over the vocab that we've observed it with.\n",
        "        for idx, word in enumerate(self.counts[context]):\n",
        "            prob = self.counts[context][word]/total\n",
        "            dist.append(prob)\n",
        "            vocab.append(word)\n",
        "\n",
        "        index = np.argmax(np.random.multinomial(1, pvals=dist))\n",
        "        return vocab[index]\n",
        "\n",
        "    def generate_sequence(self, keep_ends=True):\n",
        "        generated = [\"[START]\"] * (self.order)\n",
        "        word = None\n",
        "        while word != \"[END]\":\n",
        "            context = ' '.join(generated[-self.order:] if self.order > 0 else \"\")\n",
        "            word = self.sample(context)\n",
        "            generated.append(word)\n",
        "        if not keep_ends:\n",
        "            generated = generated[self.order:-1]\n",
        "        return \" \".join(generated)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0384b034",
      "metadata": {
        "id": "0384b034"
      },
      "source": [
        "Let's create some language models of different orders from *Pride and Prejudice*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a4502623",
      "metadata": {
        "id": "a4502623",
        "outputId": "3aba15ee-44ee-40b8-8dd6-2c6bef3bcf27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', '1', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', ',', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', '.', '[END]']\n",
            "Context:                                                    Next: Chapter\n",
            "Context:                                                    Next: 1\n",
            "Context:                                                    Next: It\n",
            "Context:                                                    Next: is\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: truth\n",
            "Context:                                                    Next: universally\n",
            "Context:                                                    Next: acknowledged\n",
            "Context:                                                    Next: ,\n",
            "Context:                                                    Next: that\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: single\n",
            "Context:                                                    Next: man\n",
            "Context:                                                    Next: in\n",
            "Context:                                                    Next: possession\n",
            "Context:                                                    Next: of\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: good\n",
            "Context:                                                    Next: fortune\n",
            "Context:                                                    Next: ,\n",
            "Context:                                                    Next: must\n",
            "Context:                                                    Next: be\n",
            "Context:                                                    Next: in\n",
            "Context:                                                    Next: want\n",
            "Context:                                                    Next: of\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: wife\n",
            "Context:                                                    Next: .\n",
            "Context:                                                    Next: [END]\n"
          ]
        }
      ],
      "source": [
        "ngram0 = NgramModel(sequences[:max_sequences], order=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "81a51b9e",
      "metadata": {
        "id": "81a51b9e",
        "outputId": "b2f428ef-016e-40b6-8186-b4d5780ebe71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[START]', 'Chapter', '1', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', ',', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', '.', '[END]']\n",
            "Context: [START]                                            Next: Chapter\n",
            "Context: Chapter                                            Next: 1\n",
            "Context: 1                                                  Next: It\n",
            "Context: It                                                 Next: is\n",
            "Context: is                                                 Next: a\n",
            "Context: a                                                  Next: truth\n",
            "Context: truth                                              Next: universally\n",
            "Context: universally                                        Next: acknowledged\n",
            "Context: acknowledged                                       Next: ,\n",
            "Context: ,                                                  Next: that\n",
            "Context: that                                               Next: a\n",
            "Context: a                                                  Next: single\n",
            "Context: single                                             Next: man\n",
            "Context: man                                                Next: in\n",
            "Context: in                                                 Next: possession\n",
            "Context: possession                                         Next: of\n",
            "Context: of                                                 Next: a\n",
            "Context: a                                                  Next: good\n",
            "Context: good                                               Next: fortune\n",
            "Context: fortune                                            Next: ,\n",
            "Context: ,                                                  Next: must\n",
            "Context: must                                               Next: be\n",
            "Context: be                                                 Next: in\n",
            "Context: in                                                 Next: want\n",
            "Context: want                                               Next: of\n",
            "Context: of                                                 Next: a\n",
            "Context: a                                                  Next: wife\n",
            "Context: wife                                               Next: .\n",
            "Context: .                                                  Next: [END]\n"
          ]
        }
      ],
      "source": [
        "ngram1 = NgramModel(sequences[:max_sequences], order=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "35ee1ead",
      "metadata": {
        "id": "35ee1ead",
        "outputId": "6841d059-b180-487a-fb3f-7e86511e07f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[START]', '[START]', 'Chapter', '1', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', ',', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', '.', '[END]']\n",
            "Context: [START] [START]                                    Next: Chapter\n",
            "Context: [START] Chapter                                    Next: 1\n",
            "Context: Chapter 1                                          Next: It\n",
            "Context: 1 It                                               Next: is\n",
            "Context: It is                                              Next: a\n",
            "Context: is a                                               Next: truth\n",
            "Context: a truth                                            Next: universally\n",
            "Context: truth universally                                  Next: acknowledged\n",
            "Context: universally acknowledged                           Next: ,\n",
            "Context: acknowledged ,                                     Next: that\n",
            "Context: , that                                             Next: a\n",
            "Context: that a                                             Next: single\n",
            "Context: a single                                           Next: man\n",
            "Context: single man                                         Next: in\n",
            "Context: man in                                             Next: possession\n",
            "Context: in possession                                      Next: of\n",
            "Context: possession of                                      Next: a\n",
            "Context: of a                                               Next: good\n",
            "Context: a good                                             Next: fortune\n",
            "Context: good fortune                                       Next: ,\n",
            "Context: fortune ,                                          Next: must\n",
            "Context: , must                                             Next: be\n",
            "Context: must be                                            Next: in\n",
            "Context: be in                                              Next: want\n",
            "Context: in want                                            Next: of\n",
            "Context: want of                                            Next: a\n",
            "Context: of a                                               Next: wife\n",
            "Context: a wife                                             Next: .\n",
            "Context: wife .                                             Next: [END]\n"
          ]
        }
      ],
      "source": [
        "ngram2 = NgramModel(sequences[:max_sequences], order=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c55ac8d",
      "metadata": {
        "id": "7c55ac8d"
      },
      "source": [
        "**Q1.** Create a `perplexity` function that can takes two arguments: a.) a model of *any* ngram order (from the class above); and b.) a sequence to calculate perplexity for.  You'll recall from class that perplexity under a particular language model for sequence $w$ is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\textrm{perplexity}_{model}(w) = \\exp\\left(-{1 \\over N} \\sum_{i=1}^N \\log P_{model}(w_i) \\right)\n",
        "$$\n",
        "\n",
        "$P_{model}(w_i)$ calculates the probability of token $w_i$ using whatever assumptions that model makes -- for a bigram model (order 1), this is $P(w_i \\mid w_{i-1})$, for a trigram model (order 2), this is $P(w_i \\mid w_{i-2}, w_{i-1})$, etc.  Two things to note:\n",
        "\n",
        "* When calculating the probability of the first word(s), be sure to get the conditioning context right.  The conditioning context for the first word in a trigram model, for example, is $P(w_i \\mid$ [START] [START]$)$.\n",
        "* Perplexity is only calculated for the words in the actual sequence.  We don't include the $P$([START]) or $P$([END]) in the perplexity calculuation.\n",
        "\n",
        "\n",
        "*Hint*: when working on this function, you might want to debug by printing out the probabilities of each $w_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1d32301c",
      "metadata": {
        "id": "1d32301c"
      },
      "outputs": [],
      "source": [
        "from math import log, exp\n",
        "\n",
        "def perplexity(model, tokens):\n",
        "    n = len(tokens)\n",
        "    start_padding = [\"[START]\"] * model.order\n",
        "    end_padding = [\"[END]\"]\n",
        "    tokens = start_padding + tokens + end_padding\n",
        "    p_sum = 0.0\n",
        "    for i in range(model.order, len(tokens) - 1): # all tokens excluding start and end\n",
        "        context = \" \".join(tokens[i - model.order:i]) if model.order > 0 else \"\"\n",
        "        word = tokens[i]\n",
        "\n",
        "        # calc P(word|context)\n",
        "        if context in model.counts and word in model.counts[context]:\n",
        "            total = sum(model.counts[context].values())\n",
        "            prob = model.counts[context][word] / total\n",
        "        else:\n",
        "            prob = 1e-10  # assign small probability to avoid 0\n",
        "\n",
        "        p_sum += log(prob)\n",
        "    return exp(-1/n * p_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8d0a8a",
      "metadata": {
        "id": "ea8d0a8a"
      },
      "source": [
        "**Q2**. Execute that perplexity function for the following language models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2a4b0e8a",
      "metadata": {
        "id": "2a4b0e8a",
        "outputId": "6fc578d9-169e-4fc6-d8c7-5b98728d744d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "202.10235240923632"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "perplexity(ngram0, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "dd2d4b64",
      "metadata": {
        "id": "dd2d4b64",
        "outputId": "08be88fe-b5b0-4f7d-c148-ac5a418b4e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.56747449844477"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "perplexity(ngram1, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "05be627d",
      "metadata": {
        "id": "05be627d",
        "outputId": "cb72d98f-153c-45ed-8678-e966bff7b39d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.416705950936633"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "perplexity(ngram2, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a331078",
      "metadata": {
        "id": "8a331078"
      },
      "source": [
        "**Q3.** What is the perplexity of \"She was a really great friend of Mr. Bingley.\" in the trigram language model trained above?\n",
        "\n",
        "Explain in 100 words what behavior is expected (and correct) given how an n-gram language model works and the data we are training it on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c0a4f81c",
      "metadata": {
        "id": "c0a4f81c",
        "outputId": "e9081373-ea61-43a3-c3fb-caa4a00603ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4044.792933650393"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "perplexity(ngram2, word_tokenize(\"She was a really great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We expect a higher perplexity value"
      ],
      "metadata": {
        "id": "Q3VDPBdNIy-m"
      },
      "id": "Q3VDPBdNIy-m"
    },
    {
      "cell_type": "markdown",
      "id": "19d3de75-a37c-4983-8f40-a8306142caca",
      "metadata": {
        "id": "19d3de75-a37c-4983-8f40-a8306142caca"
      },
      "source": [
        "**Q4.** What 1-token sequence yields the lowest perplexity for the 0-order n-gram model? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7143c48-5c08-4ada-af35-8488b0de3ffd",
      "metadata": {
        "id": "a7143c48-5c08-4ada-af35-8488b0de3ffd"
      },
      "outputs": [],
      "source": [
        "perplexity(ngram0, word_tokenize(\"YOUR_ANSWER_HERE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e059ed-8d4f-4698-b0c2-c42231a2105a",
      "metadata": {
        "id": "c4e059ed-8d4f-4698-b0c2-c42231a2105a"
      },
      "source": [
        "**Q5.** Write a function to find the n-token sequence with the lowest perplexity given an n-gram model. Explain why it should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ffb4d492-8c34-47f9-bf3f-b03ca2dfcdd3",
      "metadata": {
        "id": "ffb4d492-8c34-47f9-bf3f-b03ca2dfcdd3"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "import numpy as np\n",
        "\n",
        "def min_ppl_sequence(model, n=3):\n",
        "    \"\"\"\n",
        "    Find an n-token sequence with low perplexity using greedy search.\n",
        "\n",
        "    Args:\n",
        "        model: NgramModel instance\n",
        "        n: Length of the sequence to generate (excluding [START] and [END])\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_sequence, best_perplexity) where best_sequence is the list of tokens\n",
        "               with the lowest perplexity found, and best_perplexity is its perplexity score\n",
        "    \"\"\"\n",
        "    # Extract vocabulary from the model\n",
        "    vocab = set()\n",
        "    for context in model.counts:\n",
        "        for word in model.counts[context]:\n",
        "            vocab.add(word)\n",
        "    vocab = list(vocab - {\"[START]\", \"[END]\"})  # Exclude [START] and [END]\n",
        "\n",
        "    # Initialize sequence with [START] tokens\n",
        "    sequence = [\"[START]\"] * model.order\n",
        "    for _ in range(n):\n",
        "        # Get the current context (last model.order words or empty for order=0)\n",
        "        context = \" \".join(sequence[-model.order:]) if model.order > 0 else \"\"\n",
        "\n",
        "        # Find the word with the highest probability given the context\n",
        "        best_word = None\n",
        "        best_prob = -1\n",
        "        for word in vocab:\n",
        "            if context in model.counts and word in model.counts[context]:\n",
        "                total = sum(model.counts[context].values())\n",
        "                prob = model.counts[context][word] / total\n",
        "            else:\n",
        "                prob = 1e-10  # Small probability for unseen context-word pairs\n",
        "            if prob > best_prob:\n",
        "                best_prob = prob\n",
        "                best_word = word\n",
        "        sequence.append(best_word)\n",
        "\n",
        "    # Extract the n-token sequence (exclude [START] tokens)\n",
        "    final_sequence = sequence[model.order:]\n",
        "\n",
        "    # Calculate perplexity of the final sequence\n",
        "    perplexity_value = perplexity(model, final_sequence)\n",
        "\n",
        "    return final_sequence, perplexity_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3facf28b-323e-4f21-b736-24d707d10f6f",
      "metadata": {
        "id": "3facf28b-323e-4f21-b736-24d707d10f6f",
        "outputId": "791568bb-1459-4fbe-a30f-c9765cdc1c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3637494660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_ppl_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-89779992.py\u001b[0m in \u001b[0;36mmin_ppl_sequence\u001b[0;34m(model, n)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Calculate perplexity for the sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mperp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mperp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_perplexity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mbest_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-674680335.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model, tokens)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlog_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Exclude [END] from perplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "min_ppl_sequence(ngram1, n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f7c13d61-547c-427f-9f0e-bee7c8d27f59",
      "metadata": {
        "id": "f7c13d61-547c-427f-9f0e-bee7c8d27f59",
        "outputId": "27c34aaa-7d7e-4bc3-d034-b2383fefd494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "607.6880507020054"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# the provided call needs the tokens from above or it wont work, not the whole phrase.\n",
        "from nltk import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(min_ppl_sequence(ngram1, n=3))        # tokenize into [\"I\", \"am\", \",\" , \"but\"]\n",
        "perplexity(ngram1, tokens)\n",
        "\n",
        "#perplexity(ngram1, min_ppl_sequence(ngram1, n=3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(ngram1, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ],
      "metadata": {
        "id": "vhzFuGC9NYFz",
        "outputId": "702c41bb-3da2-4c6e-8cd6-fc12da0ffa30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vhzFuGC9NYFz",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "282.54065924895446"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}