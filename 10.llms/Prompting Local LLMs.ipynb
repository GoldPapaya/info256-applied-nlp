{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/10.llms/Prompting%20Local%20LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTwwQk96IEQe"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/10.llms/Prompting%20Local%20LLMs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM7cO8y_PKZg"
      },
      "source": [
        "In this notebook, we'll explore few-shot learning with [Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B); this model can fit within the memory and processing constraints of a T4 GPU on Google Colab while also being openly available.\n",
        "\n",
        "Then, we will also use quantization to fit a larger model ([Qwen3-14B]()) on the T4 GPU by converting the model weights to 4-bits instead of the full 16-bits.\n",
        "\n",
        "Can you create a new classification task and design prompts to differentiate between the classes within it?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwHS9LTUReib"
      },
      "outputs": [],
      "source": [
        "from textwrap import dedent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrIEtQc10uej"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPCwcrivPKZt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQlLN3_tIEQi"
      },
      "outputs": [],
      "source": [
        "# check that the GPU is available\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVRJjCNQCKH"
      },
      "source": [
        "## Qwen3-4B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOz-lch2QK49"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-4B\", device_map=\"cuda\", dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-4xaVSxO_Xb"
      },
      "outputs": [],
      "source": [
        "def classify_with_prompt(labels, shots, target_x, thinking=False):\n",
        "    system_prompt = dedent(f\"\"\"\n",
        "        You're a helpful assistant for text classification. You'll be given an input text and need to output a single choice from the following set of categories:\n",
        "        {', '.join(labels)}\n",
        "        Pick one of those labels and do not generate any other text.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": shots[0][\"X\"]}, {\"role\": \"assistant\", \"content\": shots[0][\"y\"]},\n",
        "        {\"role\": \"user\", \"content\": shots[1][\"X\"]}, {\"role\": \"assistant\", \"content\": shots[1][\"y\"]},\n",
        "        {\"role\": \"user\", \"content\": shots[2][\"X\"]}, {\"role\": \"assistant\", \"content\": shots[2][\"y\"]},\n",
        "        {\"role\": \"user\", \"content\": target_x}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=thinking # Switches between thinking and non-thinking modes. Default is True.\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # conduct text completion\n",
        "    generated = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=32768\n",
        "    )\n",
        "\n",
        "    # let's break this down:\n",
        "    #                      | we take the element of the batch (our batch size is 1)\n",
        "    #                      |  |-----------------------------| skip our original input\n",
        "    output_ids = generated[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "    # decode into token space\n",
        "    print(tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq23gAVHQhjA"
      },
      "outputs": [],
      "source": [
        "shots = [\n",
        "    {\"X\":\"I love this movie\", \"y\": \"positive\"},\n",
        "    {\"X\":\"I hate this movie\", \"y\": \"negative\"},\n",
        "    {\"X\":\"I kind of like the movie\", \"y\": \"positive\"}\n",
        "]\n",
        "\n",
        "target_x = \"This is one of the best movies I've ever seen\"\n",
        "\n",
        "classify_with_prompt([\"positive\", \"negative\"], shots, target_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz-N_AmGYp0p"
      },
      "outputs": [],
      "source": [
        "shots = [\n",
        "    {\"X\":\"Vampires take over the planet during an eclipse\", \"y\": \"Horror\"},\n",
        "    {\"X\":\"A court sentences George to be Jerry's butler\", \"y\": \"Comedy\"},\n",
        "    {\"X\":\"John turns into a werewolf during a full moon\", \"y\": \"Horror\"}\n",
        "]\n",
        "\n",
        "target_x = \"John is a werewolf who plays basketball\"\n",
        "\n",
        "classify_with_prompt([\"Horror\", \"Comedy\"], shots, target_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ2cFQIpS1Jt"
      },
      "outputs": [],
      "source": [
        "shots = [\n",
        "    {\"X\":\"This is a text\", \"y\": \"English\"},\n",
        "    {\"X\":\"Nel mezzo del cammin' di nostra vita\", \"y\": \"Italian\"},\n",
        "    {\"X\":\"Je ne sais pas\", \"y\": \"French\"},\n",
        "]\n",
        "\n",
        "target_x = \"Siempre imaginé que el Paraíso sería algún tipo de biblioteca\"\n",
        "\n",
        "classify_with_prompt([\"English\", \"Italian\", \"French\", \"Spanish\", \"Japanese\"], shots, target_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpfbqxWTPKZ4"
      },
      "source": [
        "Construct a new classification task; try to find one that the 4B model fails for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo5voJ-BIEQk"
      },
      "outputs": [],
      "source": [
        "shots = [\n",
        "    # FILL ME IN\n",
        "]\n",
        "\n",
        "target_x = \"\"\n",
        "\n",
        "classify_with_prompt([\n",
        "    # FILL ME IN\n",
        "], shots, target_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY_n3aPaPKZ4"
      },
      "source": [
        "## Qwen-14B with Quantization\n",
        "\n",
        "Now let's try a bigger model. A general rule of thumb is to multiply the model size by 4 to estimate how much GPU memory you will need for inference. For example, without quantization, a 14-billion parameter model would require roughly 56GB of memory for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVIrKUpkIEQl"
      },
      "outputs": [],
      "source": [
        "# first, delete the previous model to free up memory\n",
        "\n",
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekJxihTYIEQl"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen3-14B\",\n",
        "    device_map=\"cuda\",\n",
        "    dtype=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY9R0oSnIEQl"
      },
      "source": [
        "Rerun the prompting tasks from above. Are any of the outputs different?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKJOCOWIEQl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}