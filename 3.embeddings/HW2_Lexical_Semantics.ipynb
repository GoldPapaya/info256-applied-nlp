{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/3.embeddings/HW2_Lexical_Semantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHCHnheBcwYs"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/3.embeddings/HW2_Lexical_Semantics.ipynb)\n",
        "\n",
        "**N.B.** Once it's open on Colab, remember to save a copy (by e.g. clicking `Copy to Drive` above).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTNSdu8qV1lt"
      },
      "source": [
        "## Homework 2: Lexical Semantics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDmHpjTJziCs"
      },
      "source": [
        "In this homework, we will explore lexical semantics in the context of slang and FastText, an alternative to word2vec (Part 1); and, how to represent a sentence with individual word vectors, so we can measure the similarity between a pair of sentences (Part 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5qAYY2vxLH"
      },
      "source": [
        "### Part 1: Slang and word similarity with FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpqZQI31YG-S"
      },
      "source": [
        "Slang presents an interesting linguistic phenomenon that involves non-standard word forms. For this question, you will explore how FastText, an alternative to Word2Vec, handles the lexical semantics of slang and informal language.\n",
        "\n",
        "First, **familiarize yourself with the slang dataset that we are using**, introduced in [\"Toward Informal Language Processing: Knowledge of Slang in Large Language Models\" (Sun et al., NAACL 2024)](https://aclanthology.org/2024.naacl-long.94/). The full dataset includes annotations indicating whether a sentence from OpenSubtitles (typically a line from a movie) contains a slang term, and you can find some example sentences and terms here:\n",
        "\n",
        "https://raw.githubusercontent.com/dbamman/anlp25/main/data/slang_examples.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TORzUwVBqeJn"
      },
      "source": [
        "What are the 10 most common slang words?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/slang_examples.tsv"
      ],
      "metadata": {
        "id": "VXy-yTv9r3FO",
        "outputId": "97d1a082-8f5e-4507-e600-a949b2f6dc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-13 20:36:11--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/slang_examples.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 376405 (368K) [text/plain]\n",
            "Saving to: ‘slang_examples.tsv.3’\n",
            "\n",
            "\rslang_examples.tsv.   0%[                    ]       0  --.-KB/s               \rslang_examples.tsv. 100%[===================>] 367.58K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-09-13 20:36:11 (7.80 MB/s) - ‘slang_examples.tsv.3’ saved [376405/376405]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import split\n",
        "document = open(\"slang_examples.tsv\").read()\n",
        "slang = {}\n",
        "sentences = document.split('\\n')\n",
        "for sentence in sentences:\n",
        "    try:\n",
        "        key, value = sentence.split('\\t')\n",
        "        slang[key] = value\n",
        "    except:\n",
        "        continue # ignore badly formatted sentences\n",
        "\n",
        "freq_dict = {}\n",
        "for value in slang.values():\n",
        "    freq_dict[value] = freq_dict.get(value, 0) + 1\n",
        "\n",
        "slang_list = list(sorted(freq_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "for i in slang_list[:10]:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "nqGr5AlZ45zD",
        "outputId": "44b1e6d0-72fb-4b2e-9ba7-45337e9d3eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('gonna', 399)\n",
            "('yeah', 176)\n",
            "('shit', 116)\n",
            "('wanna', 105)\n",
            "(\"ain't\", 76)\n",
            "('gotta', 62)\n",
            "('mate', 62)\n",
            "('okay', 47)\n",
            "('man', 47)\n",
            "('kid', 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYp8KX0dqeJp"
      },
      "source": [
        "Next, **train a [FastText model](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText) using `gensim` and `FastText`** on our slang data derived from the dataset described above, which you can download here:\n",
        "\n",
        "https://raw.githubusercontent.com/dbamman/anlp25/main/data/slang_corpus.txt\n",
        "\n",
        "For preprocessing, in the `txt` file, each token is already separated by whitespace, so you don't need to worry about tokenization. Treat each line in the file as one \"sentence\". For training, use the following parameters: embedding size of 400, context window of 5, frequency threshold of 5, and use 5 workers to train for 5 epochs.\n",
        "\n",
        "Note: we are using the `FastText` _implementation_ included in the `gensim` library! **Don't use the `fasttext` library.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "EhSbAJHQ5m8X",
        "outputId": "88bbfa7e-1343-4d66-be3e-f00eb0fbb764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/slang_corpus.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "t9DhJzt16maS",
        "outputId": "fb28ebd3-da5b-4be1-d16e-7b5a76d7d613"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-13 20:36:19--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/slang_corpus.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2868710 (2.7M) [text/plain]\n",
            "Saving to: ‘slang_corpus.txt.2’\n",
            "\n",
            "\rslang_corpus.txt.2    0%[                    ]       0  --.-KB/s               \rslang_corpus.txt.2  100%[===================>]   2.74M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-09-13 20:36:19 (36.4 MB/s) - ‘slang_corpus.txt.2’ saved [2868710/2868710]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copied the file parser from WordEmbeddings.ipynb\n",
        "import re\n",
        "\n",
        "sentences=[]\n",
        "filename=\"slang_corpus.txt\"\n",
        "with open(filename) as file:\n",
        "    for line in file:\n",
        "        words = line.rstrip().lower()\n",
        "        # this file is already tokenize, so we can split on whitespace\n",
        "        # but first let's replace any sequence of whitespace (space, tab, newline, etc.) with single space\n",
        "        words = re.sub(r\"\\s+\", \" \", words)\n",
        "        sentences.append(words.split(\" \"))"
      ],
      "metadata": {
        "id": "jCR9_7Hhyd_O"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "piMQdXeRqeJp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "65d8f0de-25ce-4388-980c-2c95bc38d54c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2042983, 3344590)"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import FastText\n",
        "\n",
        "ft_model = FastText(vector_size=400, window=5, min_count=5, workers=5)\n",
        "ft_model.build_vocab(sentences)\n",
        "ft_model.train(sentences, total_examples=ft_model.corpus_count, epochs=ft_model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV57BuC_qeJr"
      },
      "source": [
        "With the trained models:\n",
        "\n",
        "**Q1.** Pick a slang term, and in about 100 words, discuss:\n",
        "\n",
        "- what the most similar words are to the slang term of your choosing (as measured by the model), and\n",
        "- whether the result is aligned with your understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "aajmAlPKqeJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a9133f0-8ccf-424c-e8c8-310cec0e147d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('yep', 0.8565532565116882),\n",
              " ('ye', 0.8376001119613647),\n",
              " ('ah', 0.8322916030883789),\n",
              " ('swell', 0.8200086951255798),\n",
              " ('uh', 0.8141594529151917)]"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "ft_model.wv.most_similar('yeah', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The five most similar words to the slang term 'yeah' are: 'yep', 'ye', 'ah', 'swell', and 'uh'. This list of words fits reasonably well with my expectations, as several of these words share the same base components as 'yeah', such as 'ye' and 'yep' containing the same first two letters. This is consistent with FastText's utilization of n-grams."
      ],
      "metadata": {
        "id": "GJc27PxtIU1w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy1QSpMktM_W"
      },
      "source": [
        "**Q2.** Train a separate word2vec (not FastText) model using the same dataset, and in about 100 words, compare the two approaches used to estimate word vectors. Here are some potential topics:\n",
        "\n",
        "- Look up the token `gonna` in both word2vec and FastText models. What does this tell you?\n",
        "- What is the high level difference between word2vec and FastText?\n",
        "- Evaluate the quality of the trained embeddings through intrinsic evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "9LWlXAz6qeJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9cedd721-e9ac-44f2-b2b7-7037a71c5c76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2043310, 3344590)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(vector_size=400, window=5, min_count=5, workers=5)\n",
        "w2v_model.build_vocab(sentences)\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar('yeah', topn=5)"
      ],
      "metadata": {
        "id": "ZAKYJkdEX7jF",
        "outputId": "e5e667eb-f7f9-4a46-9ca0-0438e196a3cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('uh', 0.8316991329193115),\n",
              " ('yes', 0.8296308517456055),\n",
              " ('okay', 0.8067767024040222),\n",
              " ('ok', 0.7860023975372314),\n",
              " ('fine', 0.7690392732620239)]"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By plugging in the same slang term 'yeah' into the Word2Vec model, we see results that differ from the FastText model by quite a lot. Most of the top five similar words for Word2Vec look like words that may come before or after 'yeah' in a sentence, but do not necessarily share the same letters or structure, as was the case with FastText. The main difference between FastText and Word2Vec visible in this example is that Word2Vec seems to base similarity more on sentence context, where FastTest does so based on the structure of the word itself."
      ],
      "metadata": {
        "id": "foMKW71W3BtW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtbVFoKquMFI"
      },
      "source": [
        "### Part 2. From words to sentences\n",
        "\n",
        "So far we've been working with word vectors, but in real-world scenarios, we often want to work with not just a word, but a sequence (like a sentence), which will explore later in the semester. However, with what we have learned so far, how do you represent a sentence? One approach is to look up the word vectors for individual words in the sentence and then *average* them, which we will explore in this question. We will be using pre-trained GloVe vectors [cf. SLP 6.8.3] we used in class. Download them here:\n",
        "\n",
        "https://raw.githubusercontent.com/dbamman/anlp25/main/data/glove.6B.100d.100K.txt\n",
        "\n",
        "**Q3.** Load the pretrained embeddings with `gensim`'s `load_word2vec_format` (see the lab notebooks), and create a function that takes a pair of sentences as input, and outputs the similarity of the two sentences measured by cosine -- the sentence pair you can use for sanity check is provided below.\n",
        "\n",
        "Find a pair of sentences where the similarity is high, but mean different (or opposite) things. Find a pair of sentences where the similarity is low, but you think the meanings are similar. In a paragraph, discuss why we might see these results given how we construct sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "ejMPHItCqeJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "204c5ddb-982a-43f3-c1c8-c662463ea684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-13 20:37:10--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/glove.6B.100d.100K.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85951834 (82M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.100K.txt.2’\n",
            "\n",
            "glove.6B.100d.100K. 100%[===================>]  81.97M   173MB/s    in 0.5s    \n",
            "\n",
            "2025-09-13 20:37:12 (173 MB/s) - ‘glove.6B.100d.100K.txt.2’ saved [85951834/85951834]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/glove.6B.100d.100K.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYiKUIr5cwYw"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.100K.txt\", binary=False, no_header=True)"
      ],
      "metadata": {
        "id": "lz9tOl7liVQN"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    s1_tokens = sentence1.lower().split()\n",
        "    s2_tokens = sentence2.lower().split()\n",
        "\n",
        "    s1_vectors = [glove_model[s1_token] for s1_token in s1_tokens if s1_token in glove_model]\n",
        "    s2_vectors = [glove_model[s2_token] for s2_token in s2_tokens if s2_token in glove_model]\n",
        "\n",
        "    if not s1_vectors or not s2_vectors:\n",
        "        return 0\n",
        "\n",
        "    avg_vector1 = np.mean(s1_vectors, axis=0)\n",
        "    avg_vector2 = np.mean(s2_vectors, axis=0)\n",
        "\n",
        "    return glove_model.cosine_similarities(avg_vector1, [avg_vector2])[0]"
      ],
      "metadata": {
        "id": "KX7bKOT9pzqE"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one=\"The queen rules the castle\"\n",
        "two=\"The king rules the castle\"\n",
        "\n",
        "sentence_similarity(one, two)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UfjMzi1qrBSc",
        "outputId": "efba830e-952d-48ba-8502-5b8e12f26874"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98152906"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "three=\"The turkey we had for supper last night was absolutely fantastic\"\n",
        "four=\"We had a really great turkey for dinner yesterday evening\"\n",
        "\n",
        "sentence_similarity(three, four)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OFvVBfBqOxSv",
        "outputId": "c24565e5-7193-405c-e8dd-262b2820a1a3"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9676"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the comparison between high-similarity different-meaning sentences and low-similarity similar-meaning sentences yielded a high cosine score. I believe that this is in large part due to how sentence inputs are interpreted by the sentence_similarity function. The input sentences are first tokenized and then vectorized by the glove model, but then to compare the two sentences, I averaged the vectors, because glove_model.cosine_similarities supports comparison between a single vector and a set of other vectors. Since the sentence embeddings in both cases were averaged and then compared, it is difficult to find distinct word vectors that are truly orthogonal to one another. Another influence is the glove model itself, as its training corpus may overemphasize situations in which two *normally* different-meaning words appear in similar contexts. We see this effect when comparing two normally independent words (such as 'computer' and 'ancient' below) in isolation - the cosine score is much lower which is expected, but still not negative. Substituting in a single word to act as a sentence also removes the 'averaging' effect mentioned previously."
      ],
      "metadata": {
        "id": "pSYCKQcJ4Se3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "five=\"computer\"\n",
        "six=\"ancient\"\n",
        "\n",
        "sentence_similarity(five, six)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1Ftv6x1OoYiI",
        "outputId": "b2e53356-56be-4d57-87a1-b966e0ee7043"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25513265"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqWFmNaBPYUq"
      },
      "source": [
        "## To submit\n",
        "\n",
        "Congratulations on finishing this homework!\n",
        "Please follow the instructions below to download the notebook file (`.ipynb`) and its printed version (`.pdf`) for submission on bCourses -- remember **all cells must be executed**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_TfNi4MzsTLk"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}