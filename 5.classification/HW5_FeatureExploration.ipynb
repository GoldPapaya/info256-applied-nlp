{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/5.classification/HW5_FeatureExploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ1_xtISD2WL"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/5.classification/HW5_FeatureExploration.ipynb)\n",
        "\n",
        "**N.B.** Once it's open on Colab, remember to save a copy (by e.g. clicking `Copy to Drive` above).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBYSYHfQ5V9g"
      },
      "source": [
        "# Feature engineering for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZD-F6iB63lv"
      },
      "source": [
        "This notebook explores feature engineering for text classification.  Your task is to create two new feature functions (like `dictionary_feature` and `unigram_feature` below), and include them in the `build_features` function.  What features do you think will help for your particular problem? Your grade is *not* tied to whether accuracy goes up or down, so be creative!  You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajlr1qNk63lx"
      },
      "source": [
        "You are free to use any of the following datasets for this exercise, or to use your own (if you have your own labeled data with at least 500 examples from at least two classes, I would encourage you to use it!).  If you use your own data, just be sure to format it like the examples below; each directory has a `train.tsv`, `dev.tsv` and `test.tsv` file, where each file is tab-separated (label in the first column and text in the second column).\n",
        "\n",
        "* [Sentiment Analysis](https://ai.stanford.edu/~amaas/data/sentiment/) (Positive/Negative)\n",
        "* [Congressional Speech](https://www.cs.cornell.edu/home/llee/data/convote.html) (Democrat/Republican)\n",
        "* Library of Congress Subject Classication ([21 categories](https://en.wikipedia.org/wiki/Library_of_Congress_Classification))\n",
        "\n",
        "For whichever dataset you pick, download the data first using the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNh-QjKm67VB"
      },
      "outputs": [],
      "source": [
        "# get LMRD data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/train.tsv -O lmrd_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/dev.tsv -O lmrd_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/test.tsv -O lmrd_test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fJd1C1cu7QD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646ffbde-a87e-4c49-cfa0-e549931dfd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-26 16:42:11--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4660140 (4.4M) [text/plain]\n",
            "Saving to: ‘convote_train.tsv’\n",
            "\n",
            "convote_train.tsv   100%[===================>]   4.44M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-26 16:42:12 (126 MB/s) - ‘convote_train.tsv’ saved [4660140/4660140]\n",
            "\n",
            "--2025-09-26 16:42:12--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 351382 (343K) [text/plain]\n",
            "Saving to: ‘convote_dev.tsv’\n",
            "\n",
            "convote_dev.tsv     100%[===================>] 343.15K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-09-26 16:42:12 (23.1 MB/s) - ‘convote_dev.tsv’ saved [351382/351382]\n",
            "\n",
            "--2025-09-26 16:42:12--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1707975 (1.6M) [text/plain]\n",
            "Saving to: ‘convote_test.tsv’\n",
            "\n",
            "convote_test.tsv    100%[===================>]   1.63M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-26 16:42:13 (61.0 MB/s) - ‘convote_test.tsv’ saved [1707975/1707975]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get Convote data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/train.tsv -O convote_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/dev.tsv -O convote_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/test.tsv -O convote_test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2kamLx07T6x"
      },
      "outputs": [],
      "source": [
        "# get LoC data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/loc/train.tsv -O loc_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/loc/dev.tsv -O loc_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/loc/test.tsv -O loc_test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wh28crHR63ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d124627a-099e-4e2b-e223-2fb6a84d9dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from sklearn import linear_model, preprocessing\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaiV9Dy763lx"
      },
      "source": [
        "## Part 1: Loading data\n",
        "\n",
        "**Q1: Briefly describe your data (including the categories you're predicting).**  If you're using your own data, tell us about it; if you're using one of the datasets above, tell us something that shows you've looked at the data. How many examples are in each category?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8kDz__uf63lz"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "    df = pd.read_csv(filename, names=[\"label\", \"text\"], sep=\"\\t\")\n",
        "    return df.text.to_list(), df.label.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ej8Ge3bR63lz"
      },
      "outputs": [],
      "source": [
        "# Change this to the directory with the data you will be using.\n",
        "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
        "data = \"convote\"\n",
        "\n",
        "x_train, y_train = read_data(\"%s_train.tsv\" % data)\n",
        "x_dev, y_dev = read_data(\"%s_dev.tsv\" % data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJN8NcpK5V9o",
        "outputId": "5360fc42-618a-4785-a8c8-1d01b477f6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mr. speaker , i rise in opposition to the rules package that we have before us today .  it is outrageous that my republican colleagues have placed before us a rules package that at best lacks integrity , and at worst is completely unethical .  as the highest body of elected officials in our country , we should be held to the highest ethical standards .  but instead , my republican colleagues have opted to put before us a rules package that actually lowers our ethics standards , so that they may promote their own agenda , at whatever cost .  this rules package makes it far more difficult for ethics investigations to take place .  by requiring a majority of the ethics committee before an investigation can even begin , we are in great danger of diminishing the integrity of our great institution .  with this new rule , the majority party can effectively block any ethics investigation of a member of their party .  this is an abuse of power .  and it 's not just democrats who oppose this plan .  americans across the country have expressed their opposition to this plan .  my democratic colleagues and i have a better plan that will strengthen the ethics rules to improve congressional accountability and to make sure that legislation is properly considered .  the republican plan fails to close a loophole that allows legislation to be considered before members have read it .  last year this led to the passage of a provision that would have let the federal government deeply invade citizens ' privacy by reading their tax returns .  i am appalled that the republicans have failed to include the democratic provision to tighten this loophole .  mr. speaker , i urge my colleagues to vote `` no '' on the resolution , so that we do not allow this rules package to become law .  \n",
            "D\n",
            "\n",
            "mr. chairman , i thank the gentlewoman for yielding me this time .  my good colleague from california raised the exact and critical point .  the question is , what happens during those 45 days ?  we will need to support elections .  there is not a single member of this house who has not supported some form of general election , a special election , to replace the members at some point .  but during that 45 days , what happens ?  the chair of the constitution subcommittee says this is what happens : martial law .  we do not know who would fill the vacancy of the presidency , but we do know that the succession act most likely suggests it would be an unelected person .  the sponsors of the bill before us today insist , and i think rightfully so , on the importance of elections .  but to then say that during a 45-day period we would have none of the checks and balances so fundamental to our constitution , none of the separation of powers , and that the presidency would be filled by an unelected member of the cabinet who not a single member of this country , not a single citizen , voted to fill that position , and that that person would have no checks and balances from congress for a period of 45 days i find extraordinary .  i find it inconsistent .  i find it illogical , and , frankly , i find it dangerous .  the gentleman from wisconsin refused earlier to yield time , but i was going to ask him , if virginia has those elections in a shorter time period , they should be commended for that .  so now we have a situation in the congress where the virginia delegation has sent their members here , but many other states do not have members here .  do they at that point elect a speaker of the house in the absence of other members ?  and then three more states elect their representatives , temporary replacements , or full replacements at that point .  they come in .  do they elect a new speaker ?  and if that happens , who becomes the president under the succession act ?  this bill does not address that question .  this bill responds to real threats with fantasies .  it responds with the fantasy , first of all , that a lot of people will still survive ; but we have no guarantee of that .  it responds with the fantasy that those who do survive will do the right thing .  we are here having this debate , we have debates every day , because people differ on what the right thing is to do .  i have been in very traumatic situations with people in severe car wrecks and mountain climbing accidents .  my experience has not been that crisis imbues universal sagacity and fairness .  it has not been that .  people respond in extraordinary ways , and we must preserve an institution that has the deliberative body and the checks and balances to meet those challenges .  many of our states are going increasingly to mail-in ballots .  we in this body were effectively disabled by an anthrax attack not long after september 11 .  i would ask my dear friends , will you conduct this election in 45 days if there is anthrax in the mail and still preserve the franchise of the american people ?  how will you do that ?  you have no answer to that question .  i find it extraordinary , frankly , that while saying you do not want to amend the constitution , we began this very congress by amending the constitution through the rule , by undermining the principle that a quorum is 50 percent of the body and instead saying it is however many people survive .  and if that rule applies , who will designate it , who will implement it ?  the speaker , or the speaker 's designee ?  again , not an elected person , as you say is so critical and i believe is critical , but a temporary appointee , frankly , who not a single other member of this body knows who they are .  so we not only have an unelected person , we have an unknown person who will convene this body , and who , by the way , could conceivably convene it for their own election to then become the president of the united states under the succession act .  you have refused steadfastly to debate this real issue broadly .  you had a mock debate in the committee on the judiciary in which the distinguished chairman presented my bill without allowing me the courtesy or dignity to defend it myself .  and on that , you proudly say you defend democracy .  sir , i think you dissemble in that regard .  here is the fundamental question for us , my friends , and it is this : the american people are watching television and an announcement comes on and says the congress has been destroyed in a nuclear attack , the president and vice president are killed and the supreme court is dead and thousands of our citizens in this town are .  what happens next ?  under your bill , 45 days of chaos .  apparently , according to the committee on the judiciary subcommittee on the constitution chairman , 45 days of marshal law , rule of this country by an unelected president with no checks and balances .  or an alternative , an alternative which says quite simply that the people have entrusted the representatives they send here to make profound decisions , war , taxation , a host of other things , and those representatives would have the power under the bill of the gentleman from california ( mr. rohrabacher )  bill or mine to designate temporary successors , temporary , only until we can have a real election .  the american people , in one scenario , are told we do not know who is going to run the country , we have no representatives ; where in another you will have temporary representatives carrying your interests to this great body while we deliberate and have real elections .  that is the choice .  you are making the wrong choice today if you think you have solved this problem .  \n",
            "D\n",
            "\n",
            "Train:  Counter({'R': 1373, 'D': 1350})\n",
            "Dev:  Counter({'R': 130, 'D': 127})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(x_train[0])\n",
        "print(y_train[0] + \"\\n\")\n",
        "print(x_dev[0])\n",
        "print(y_dev[0] + \"\\n\")\n",
        "print(\"Train: \", Counter(y_train))\n",
        "print(\"Dev: \", Counter(y_dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These data are of congressional debate speeches, containing a label (\"R\" or \"D\") and a speech (transcribed to text), seperated by tab. There are 1373 passages labelled R and 1350 passages labelled D in the training data, and 130 passages labelled R and 127 labelled D in the dev data, as shown above."
      ],
      "metadata": {
        "id": "aQh5h3JrE9Cq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CC2VSwQ5V9o"
      },
      "source": [
        "## Part 2: Features\n",
        "\n",
        "Here, you will hand-engineer some features for your classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0l-v6WCl63l0"
      },
      "outputs": [],
      "source": [
        "## HELPER FUNCTIONS ##\n",
        "\n",
        "def majority_class(y_train, y_dev):\n",
        "    label_counts = Counter(y_train)\n",
        "    majority = label_counts.most_common(1)[0][0]\n",
        "\n",
        "    correct = 0.\n",
        "    for label in y_dev:\n",
        "        if label == majority:\n",
        "            correct += 1\n",
        "\n",
        "    print(\"%s\\t%.3f\" % (majority, correct/len(y_dev)))\n",
        "    return correct / len(y_dev)\n",
        "\n",
        "def build_features(x_train, feature_functions):\n",
        "    data = []\n",
        "    for doc in x_train:\n",
        "        feats = {}\n",
        "        tokens = doc.split(\" \")\n",
        "\n",
        "        for function in feature_functions:\n",
        "            feats.update(function(tokens))\n",
        "\n",
        "        data.append(feats)\n",
        "    return data\n",
        "\n",
        "# This helper function converts a dictionary of feature names to unique numerical ids\n",
        "def create_vocab(data):\n",
        "    feature_vocab = {}\n",
        "    idx = 0\n",
        "    for doc in data:\n",
        "        for feat in doc:\n",
        "            if feat not in feature_vocab:\n",
        "                feature_vocab[feat] = idx\n",
        "                idx += 1\n",
        "\n",
        "    return feature_vocab\n",
        "\n",
        "# This helper function converts a dictionary of feature names to a sparse representation\n",
        "# that we can fit in a scikit-learn model.  This is important because almost all feature\n",
        "# values will be 0 for most documents (note: why?), and we don't want to save them all in\n",
        "# memory.\n",
        "\n",
        "def features_to_ids(data, feature_vocab):\n",
        "    new_data = sparse.lil_matrix((len(data), len(feature_vocab)))\n",
        "    for idx,doc in enumerate(data):\n",
        "        for f in doc:\n",
        "            if f in feature_vocab:\n",
        "                new_data[idx, feature_vocab[f]] = doc[f]\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPtsXae_63l0"
      },
      "source": [
        "We'll start with two feature classes -- one feature class noting the presence of a word in an external dictionary, and one feature class for the word identity (i.e., unigram).  We'll implement each feature class as a function that takes a single document as input (as a list of tokens) and returns a dict corresponding to the feature we're creating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m7wpOrJ563l1"
      },
      "outputs": [],
      "source": [
        "# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n",
        "\n",
        "# EDIT TO FIT YOUR DATASET (this already sort of fits the convote data, so I\n",
        "# just added to it)\n",
        "dem_dictionary = set([\"republican\",\"cut\", \"opposition\", \"programs\", \"spending\"])\n",
        "repub_dictionary = set([\"growth\",\"economy\", \"budget\", \"business\"])\n",
        "\n",
        "def political_dictionary_feature(tokens):\n",
        "    feats = {}\n",
        "    for word in tokens:\n",
        "        if word in dem_dictionary:\n",
        "            feats[\"word_in_dem_dictionary\"] = 1\n",
        "        if word in repub_dictionary:\n",
        "            feats[\"word_in_repub_dictionary\"] = 1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wtO-foMj63l1"
      },
      "outputs": [],
      "source": [
        "def unigram_feature(tokens):\n",
        "    feats = {}\n",
        "    for word in tokens:\n",
        "        feats[\"UNIGRAM_%s\" % word] = 1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7yZfUkU63l1"
      },
      "source": [
        "**Q2**: **Add first new feature function here.**  Describe your feature and why you think it will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PAw7PO_w63l1"
      },
      "outputs": [],
      "source": [
        "# finds word combinations like \"tax_cut\" or \"illegal_aliens\"\n",
        "def bigram_feature(tokens):\n",
        "    feats={}\n",
        "    for i in range(len(tokens)-1):\n",
        "      feats[\"BIGRAM_%s\" % tokens[i] + \"_\" + tokens[i+1]] = 1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GLI_RtJ63l2"
      },
      "source": [
        "**Q3**: **Add second new feature function here.** Describe your feature and why you think it will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jNcqQIx863l2"
      },
      "outputs": [],
      "source": [
        "def document_length_feature(tokens):\n",
        "    feats={}\n",
        "    feats[\"document_length\"] = len(tokens)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07dK4NPp63l2"
      },
      "source": [
        "We use the `build_features` helper function to aggregate together all of the information from different feature classes.  Each document has a feature dict (`feats`), and we'll update that dict with the new dict that each separate feature class is returning.  (Here you want to make sure that the keys each feature function is creating are unique so they don't get clobbered by other functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RMh8GzFm63l2"
      },
      "outputs": [],
      "source": [
        "# This function trains a model and returns the predicted and true labels for test data\n",
        "def evaluate(x_train, x_dev, y_train, y_dev, feature_functions):\n",
        "    x_train_feat = build_features(x_train, feature_functions)\n",
        "    x_dev_feat = build_features(x_dev, feature_functions)\n",
        "\n",
        "    # just create vocabulary from features in *training* data\n",
        "    feature_vocab = create_vocab(x_train_feat)\n",
        "\n",
        "    x_train_ids = features_to_ids(x_train_feat, feature_vocab)\n",
        "    x_dev_ids = features_to_ids(x_dev_feat, feature_vocab)\n",
        "\n",
        "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
        "    logreg.fit(x_train_ids, y_train)\n",
        "    predictions = logreg.predict(x_dev_ids)\n",
        "    #return (predictions, y_dev)\n",
        "    return logreg, feature_vocab, predictions, y_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zH7bPWTA63l3"
      },
      "outputs": [],
      "source": [
        "def print_weights(clf, vocab, n=10):\n",
        "    reverse_vocab = [None]*len(clf.coef_[0])\n",
        "    for k in vocab:\n",
        "        reverse_vocab[vocab[k]] = k\n",
        "\n",
        "    if len(clf.classes_) == 2:\n",
        "\n",
        "        weights=clf.coef_[0]\n",
        "        for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
        "            print(\"%.3f\\t%s\" % (weight, feature))\n",
        "\n",
        "        print()\n",
        "\n",
        "        for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "            print(\"%.3f\\t%s\" % (weight, feature))\n",
        "\n",
        "    else:\n",
        "        for i, cat in enumerate(clf.classes_):\n",
        "\n",
        "            weights=clf.coef_[i]\n",
        "\n",
        "            for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CFfW35rB63l3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd65492a-8ff1-44cf-9a48-246e84825adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R\t0.506\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5058365758754864"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "majority_class(y_train,y_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9jWjaT63l3"
      },
      "source": [
        "Explore the impact of different feature functions by evaluating them below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u2ci2NKi63l3"
      },
      "outputs": [],
      "source": [
        "features = [unigram_feature]\n",
        "#clf, vocab = pipeline(x_train, x_dev, y_train, y_dev, features)\n",
        "clf, vocab, predictions, y_dev = evaluate(x_train, x_dev, y_train, y_dev, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXsGfRng63l3"
      },
      "source": [
        "If you want to print the coefficients for any of the models you train, you can do so like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Pzzj7s4Q63l3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b7fe31-0b54-4aad-a7ee-925cc3fd942d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.489\tUNIGRAM_leader\n",
            "-1.307\tUNIGRAM_objection\n",
            "-1.193\tUNIGRAM_cuts\n",
            "-1.137\tUNIGRAM_present\n",
            "-1.091\tUNIGRAM_republican\n",
            "-0.967\tUNIGRAM_quorum\n",
            "-0.941\tUNIGRAM_remainder\n",
            "-0.936\tUNIGRAM_although\n",
            "-0.930\tUNIGRAM_request\n",
            "-0.909\tUNIGRAM_democratic\n",
            "\n",
            "1.067\tUNIGRAM_respond\n",
            "0.941\tUNIGRAM_taxes\n",
            "0.937\tUNIGRAM_understanding\n",
            "0.919\tUNIGRAM_bringing\n",
            "0.904\tUNIGRAM_call\n",
            "0.901\tUNIGRAM_immediate\n",
            "0.899\tUNIGRAM_absolutely\n",
            "0.873\tUNIGRAM_30\n",
            "0.866\tUNIGRAM_detained\n",
            "0.844\tUNIGRAM_yielding\n"
          ]
        }
      ],
      "source": [
        "print_weights(clf, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVwYUGla63l4"
      },
      "source": [
        "## Part 3: Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIRDXx6L5V9r"
      },
      "source": [
        "**Q4**: Implement a function that returns the parametric confidence interval bounds for a binomial estimator of the model accuracy. It should return a tuple of floats `(lower_bound, upper_bound)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDkVVU1Y5V9r"
      },
      "outputs": [],
      "source": [
        "def binomial_test(predictions, targets, significance_level=0.95):\n",
        "    # YOUR CODE HERE\n",
        "    upper_bound = 0.0\n",
        "    lower_bound = 0.0\n",
        "    return (lower_bound, upper_bound)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYZkouY05V9s"
      },
      "source": [
        "**Q5**: Plot the performance for models trained with different combinations of features, including your two custom features. Some combinations you might try (but feel free to pick your own!):\n",
        "1. Just the dictionary features\n",
        "2. Just the unigram features\n",
        "3. Just your custom features\n",
        "4. Unigram features + custom features\n",
        "\n",
        "Make a bar plot with confidence intervals. Does incorporating your features result in a statistically significant change in performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDKnZ8V263l3"
      },
      "outputs": [],
      "source": [
        "# example of how to train a classifier with the dictionary and unigram features\n",
        "features = [political_dictionary_feature, unigram_feature]\n",
        "predictions, targets = evaluate(x_train, x_dev, y_train, y_dev, features)\n",
        "lower_bound, upper_bound = binomial_test(predictions, targets)\n",
        "\n",
        "# YOUR CODE TO EVALUATE MULTIPLE MODELS HERE\n",
        "\n",
        "# YOUR CODE TO GENERATE PLOT HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFfAJs8FEDc-"
      },
      "source": [
        "---\n",
        "\n",
        "## To submit\n",
        "\n",
        "Congratulations on finishing this homework!\n",
        "Please follow the instructions below to download the notebook file (`.ipynb`) and its printed version (`.pdf`) for submission on bCourses -- remember **all cells must be executed**.\n",
        "\n",
        "1.  Download a copy of the notebook file: `File > Download > Download .ipynb`.\n",
        "\n",
        "2.  Print the notebook as PDF (via your browser, or tools like [nbconvert](https://nbconvert.readthedocs.io/en/latest/))."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}