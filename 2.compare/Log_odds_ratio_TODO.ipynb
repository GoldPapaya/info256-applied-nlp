{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/2.compare/Log_odds_ratio_TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGOJIv5Xv2PR"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/2.compare/Log_odds_ratio_TODO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdY9gXc3v2PS"
      },
      "source": [
        "# Log odds-ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVpdSX0E9Tw"
      },
      "source": [
        "The log odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
        "\n",
        "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
        "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
        "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
        "\n",
        "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sezbdfE9Ty"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset.\n",
        "   \n",
        "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\"\n",
        "\n",
        "**Describe each of those datasets and their source in 100-200 words.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtwPan7Kv2PV"
      },
      "source": [
        "Type your response here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4_1a4EE9Ty"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure dependencies are installed\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWfjo_IYxZ4n",
        "outputId": "7da66a01-624e-4c4d-9ef5-0d63aa25cd63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6eYQtYiCE9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb408178-7a67-4057-eb2c-19f4e05627bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def read_and_tokenize(filename: str) -> list[str]:\n",
        "    \"\"\"Read the file and output a list of strings (tokens).\"\"\"\n",
        "    document = open(filename).read()\n",
        "    return word_tokenize(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z28xc1Y1E9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf0aa25-2884-400c-b4df-c0d812994817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-05 19:04:50--  https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class1_dataset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112470 (110K) [text/plain]\n",
            "Saving to: ‘class1_dataset.txt’\n",
            "\n",
            "\rclass1_dataset.txt    0%[                    ]       0  --.-KB/s               \rclass1_dataset.txt  100%[===================>] 109.83K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-05 19:04:50 (4.61 MB/s) - ‘class1_dataset.txt’ saved [112470/112470]\n",
            "\n",
            "--2025-09-05 19:04:50--  https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class2_dataset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86591 (85K) [text/plain]\n",
            "Saving to: ‘class2_dataset.txt’\n",
            "\n",
            "class2_dataset.txt  100%[===================>]  84.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-05 19:04:50 (3.25 MB/s) - ‘class2_dataset.txt’ saved [86591/86591]\n",
            "\n",
            "['CAPTION', ':', '“', 'PROMETHEUS', 'STOLE', 'FIRE', 'FROM', 'THE', 'GODS', 'AND'] 25036 3431\n",
            "['[', 'narrator', ']', 'Since', 'the', 'beginning', 'of', 'time', ',', 'since'] 21395 2509\n"
          ]
        }
      ],
      "source": [
        "# change these file paths to wherever the datasets you created above live.\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class1_dataset.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class2_dataset.txt\n",
        "\n",
        "class1_tokens = read_and_tokenize(\"class1_dataset.txt\")\n",
        "class2_tokens = read_and_tokenize(\"class2_dataset.txt\")\n",
        "\n",
        "print(class1_tokens[:10], len(class1_tokens), len(set(class1_tokens))) # Oppenheimer output: sample set of tokens, # of tokens, # of unique tokens\n",
        "print(class2_tokens[:10], len(class2_tokens), len(set(class2_tokens))) # barbie output: sample set of tokens, # of tokens, # of unique tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHj4k4tE9Tz"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior. This value, $\\widehat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\widehat{\\zeta}_w^{(i-j)}= {\\widehat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\widehat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
        "* $\\alpha_w$ = 0.01\n",
        "* $V$ = size of vocabulary (number of distinct word types)\n",
        "* $\\alpha_0 = V * \\alpha_w$\n",
        "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
        "\n",
        "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lHWahiy8E9T0"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def logodds_with_uninformative_prior(tokens_i: list[str], tokens_j: list[str], display=25):\n",
        "    n_i = len(class1_tokens) # number of tokens in i\n",
        "    n_j = len(class2_tokens) # number of tokens in j\n",
        "    counter_i = Counter(tokens_i) # freq dict of i\n",
        "    counter_j = Counter(tokens_j) # freq dict of j\n",
        "    vocabulary = set(class1_tokens).union(set(class2_tokens)) # union of all types between i and j\n",
        "    V = len(vocabulary) # size of vocab\n",
        "    a_w = 0.01\n",
        "    a_0 = V * a_w\n",
        "\n",
        "    z_scores = {}\n",
        "    for word in vocabulary:\n",
        "      y_i_w = counter_i.get(word, 0) # get word frequency from i corpus\n",
        "      y_j_w = counter_j.get(word, 0) # get word frequency from j corpus\n",
        "\n",
        "      # Compute log-odds\n",
        "      log_odds = math.log((y_i_w + a_w)/(n_i + a_0 - y_i_w - a_w)) - math.log((y_j_w + a_w)/(n_j + a_0 - y_j_w - a_w))\n",
        "\n",
        "      # Compute variance\n",
        "      variance = (1/(y_i_w + a_w)) + (1/(y_j_w + a_w))\n",
        "\n",
        "      # Compute Dirichlet prior\n",
        "      z = log_odds / math.sqrt(variance)\n",
        "      z_scores[word] = z\n",
        "\n",
        "    sorted_tokens = sorted(z_scores.items(), key=lambda x: x[1], reverse=True) # sort words by their respective z score\n",
        "\n",
        "    print(\"Top 25 words for class1_dataset\")\n",
        "    count = 0\n",
        "    for word, z in sorted_tokens:\n",
        "        if z > 0:\n",
        "            print(word + \": \", z)\n",
        "            count += 1\n",
        "            if count >= 25:\n",
        "                break\n",
        "\n",
        "    print(\"\\nTop 25 words for class2_dataset\")\n",
        "    count = 0\n",
        "    for word, z in sorted_tokens[::-1]:  # Reverse for negative z-scores\n",
        "        if z < 0:\n",
        "            print(word + \": \", z)\n",
        "            count += 1\n",
        "            if count >= 25:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ddu4uK9pE9T0",
        "outputId": "4d4ce534-3ceb-42c0-e4e4-5125834678b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words for class1_dataset (Oppenheimer)\n",
            "the:  10.064142067493927\n",
            "of:  6.768086753088975\n",
            "he:  6.289930893918219\n",
            "?:  5.956014891113073\n",
            "to:  5.591600668531297\n",
            "was:  5.39462564283176\n",
            "him:  4.973958529530766\n",
            "they:  4.640397871542293\n",
            "would:  4.576687782598468\n",
            "Well:  4.503544585516553\n",
            "in:  4.011284631740978\n",
            "a:  3.990008663673083\n",
            "Robert:  3.9506739868060587\n",
            "He:  3.9125780273982818\n",
            "his:  3.863951678016242\n",
            "not:  3.7066584920983874\n",
            ",:  3.6217088345587807\n",
            "as:  3.5963275578172236\n",
            "A:  3.5278168143073674\n",
            "security:  3.444404526915689\n",
            "d:  3.4318219381896022\n",
            "did:  3.4310423287508907\n",
            "Los:  3.4217149128625826\n",
            "from:  3.386773202527786\n",
            "years:  3.3077213926653095\n",
            "\n",
            "Top 25 words for class2_dataset (Barbie)\n",
            "!:  -10.485184966722821\n",
            "]:  -8.46823701206473\n",
            "[:  -8.46823701206473\n",
            "Oh:  -7.290217775430189\n",
            "Okay:  -5.404450086769138\n",
            "Yeah:  -5.290099811527188\n",
            "Hi:  -5.242802088669298\n",
            "so:  -5.141991079066152\n",
            "just:  -5.106285554729222\n",
            "na:  -4.656812020231174\n",
            "I:  -4.654593110005942\n",
            "her:  -4.446300676104932\n",
            "m:  -4.210478781130174\n",
            "music:  -4.183745703205045\n",
            "playing:  -4.120810874061146\n",
            "night:  -4.118043278916242\n",
            "And:  -3.9787114290875327\n",
            "love:  -3.9564735066271512\n",
            "like:  -3.7846467967278032\n",
            "go:  -3.5679131679428524\n",
            "She:  -3.5447550679958617\n",
            "World:  -3.4622157151895023\n",
            "got:  -3.432178401605065\n",
            "she:  -3.4015655672947473\n",
            "Hey:  -3.398518326503109\n"
          ]
        }
      ],
      "source": [
        "logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxGYvttDv2Pc"
      },
      "source": [
        "To check your work, you can run log-odds on the party platforms from the lab section. With `nltk.word_tokenize` _before_ lower-casing, these should be your top 5 words (and scores, roughly). Depending on your tokenization strategy, your scores might be slightly different.\n",
        "\n",
        "**Democrat**:\n",
        "```\n",
        "president:\t4.75\n",
        "biden:\t4.27\n",
        "to:\t4.11\n",
        "he:\t4.09\n",
        "has:\t4.08\n",
        "```\n",
        "**Republican**\n",
        "```\n",
        "republicans:\t-13.45\n",
        "our:\t-11.23\n",
        "will:\t-10.88\n",
        "american:\t-10.01\n",
        "restore:\t-7.97\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FDnm5YrXv2Pc",
        "outputId": "57654e74-3906-4bef-b60e-1b0b2d122274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-05 19:29:55--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283046 (276K) [text/plain]\n",
            "Saving to: ‘2024_democrat_party_platform.txt’\n",
            "\n",
            "\r          2024_demo   0%[                    ]       0  --.-KB/s               \r2024_democrat_party 100%[===================>] 276.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-05 19:29:56 (7.30 MB/s) - ‘2024_democrat_party_platform.txt’ saved [283046/283046]\n",
            "\n",
            "--2025-09-05 19:29:56--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35319 (34K) [text/plain]\n",
            "Saving to: ‘2024_republican_party_platform.txt’\n",
            "\n",
            "2024_republican_par 100%[===================>]  34.49K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-09-05 19:29:56 (3.36 MB/s) - ‘2024_republican_party_platform.txt’ saved [35319/35319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I10MrjXyv2Pd",
        "outputId": "7426b612-3ba0-4cee-d929-71efcefbd18d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words for class1_dataset (Oppenheimer)\n",
            "\n",
            "Top 25 words for class2_dataset (Barbie)\n",
            "by:  -0.011088922309045972\n",
            "Monica:  -0.011088922309045972\n",
            "finally:  -0.011088922309045972\n",
            "technicalities:  -0.011088922309045972\n",
            "York:  -0.011088922309045972\n",
            "Bet:  -0.011088922309045972\n",
            "Draw:  -0.011088922309045972\n",
            "deep:  -0.011088922309045972\n",
            "allow:  -0.011088922309045972\n",
            "Boo-Boo:  -0.011088922309045972\n",
            "brave:  -0.011088922309045972\n",
            "Mermaids:  -0.011088922309045972\n",
            "bishop:  -0.011088922309045972\n",
            "policy…:  -0.011088922309045972\n",
            "sciences:  -0.011088922309045972\n",
            "peelin:  -0.011088922309045972\n",
            "reality-challenged:  -0.011088922309045972\n",
            "Stay:  -0.011088922309045972\n",
            "saying…:  -0.011088922309045972\n",
            "motoring:  -0.011088922309045972\n",
            "survival:  -0.011088922309045972\n",
            "afternoons:  -0.011088922309045972\n",
            "seconds:  -0.011088922309045972\n",
            "She:  -0.011088922309045972\n",
            "triumph:  -0.011088922309045972\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "logodds_with_uninformative_prior(\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_democrat_party_platform.txt\")],\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_republican_party_platform.txt\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXhC_JlIv2Pd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}