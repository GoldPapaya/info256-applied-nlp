{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/2.compare/Log_odds_ratio_TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGOJIv5Xv2PR"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/2.compare/Log_odds_ratio_TODO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdY9gXc3v2PS"
      },
      "source": [
        "# Log odds-ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVpdSX0E9Tw"
      },
      "source": [
        "The log odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
        "\n",
        "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
        "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
        "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
        "\n",
        "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sezbdfE9Ty"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset.\n",
        "   \n",
        "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\"\n",
        "\n",
        "**Describe each of those datasets and their source in 100-200 words.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtwPan7Kv2PV"
      },
      "source": [
        "Type your response here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4_1a4EE9Ty"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure dependencies are installed\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "tWfjo_IYxZ4n",
        "outputId": "cade3ee1-2c0b-465f-a082-61e1e22f48dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6eYQtYiCE9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314ac7b3-c4cc-4225-b0e9-2d1aacd62a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def read_and_tokenize(filename: str) -> list[str]:\n",
        "    \"\"\"Read the file and output a list of strings (tokens).\"\"\"\n",
        "    document = open(filename).read()\n",
        "    return word_tokenize(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "z28xc1Y1E9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c228e1-17b3-40c3-d7a2-aad9b59584f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-05 17:55:21--  https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class1_dataset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112470 (110K) [text/plain]\n",
            "Saving to: ‘class1_dataset.txt.9’\n",
            "\n",
            "\rclass1_dataset.txt.   0%[                    ]       0  --.-KB/s               \rclass1_dataset.txt. 100%[===================>] 109.83K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-05 17:55:21 (4.43 MB/s) - ‘class1_dataset.txt.9’ saved [112470/112470]\n",
            "\n",
            "--2025-09-05 17:55:21--  https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class2_dataset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86591 (85K) [text/plain]\n",
            "Saving to: ‘class2_dataset.txt.9’\n",
            "\n",
            "class2_dataset.txt. 100%[===================>]  84.56K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-05 17:55:21 (3.80 MB/s) - ‘class2_dataset.txt.9’ saved [86591/86591]\n",
            "\n",
            "['OPPENHEIMER', 'by', 'Christopher', 'Nolan', 'based', 'on', 'the', 'book', 'AMERICAN', 'PROMETHEUS'] 43070 5898\n",
            "['BARBIE', 'Written', 'by', 'Greta', 'Gerwig', '&', 'Noah', 'Baumbach', 'EXT', '.'] 28221 3991\n"
          ]
        }
      ],
      "source": [
        "# change these file paths to wherever the datasets you created above live.\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class1_dataset.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class2_dataset.txt\n",
        "\n",
        "class1_tokens = read_and_tokenize(\"class1_dataset.txt\")\n",
        "class2_tokens = read_and_tokenize(\"class2_dataset.txt\")\n",
        "\n",
        "print(class1_tokens[:10], len(class1_tokens), len(set(class1_tokens))) # Oppenheimer output: sample set of tokens, # of tokens, # of unique tokens\n",
        "print(class2_tokens[:10], len(class2_tokens), len(set(class2_tokens))) # barbie output: sample set of tokens, # of tokens, # of unique tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHj4k4tE9Tz"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior. This value, $\\widehat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\widehat{\\zeta}_w^{(i-j)}= {\\widehat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\widehat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
        "* $\\alpha_w$ = 0.01\n",
        "* $V$ = size of vocabulary (number of distinct word types)\n",
        "* $\\alpha_0 = V * \\alpha_w$\n",
        "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
        "\n",
        "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lHWahiy8E9T0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6357acc5-4be0-4a02-ea1d-2d3efbcaee6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def logodds_with_uninformative_prior(tokens_i: list[str], tokens_j: list[str], display=25):\n",
        "    \"\"\"Print out the log odds results given two lists of tokens.\"\"\"\n",
        "    n_i = len(class1_tokens) # number of tokens in i\n",
        "    n_j = len(class2_tokens) # number of tokens in j\n",
        "    counter_i = Counter(tokens_i) # freq dict of i\n",
        "    counter_j = Counter(tokens_j) # freq dict of j\n",
        "    vocabulary = set(class1_tokens).union(set(class2_tokens)) # union of all types between i and j\n",
        "    V = len(vocabulary) # size of vocab\n",
        "    a_w = 0.01\n",
        "    a_0 = V * a_w\n",
        "\n",
        "    z_scores = {}\n",
        "    for word in vocabulary:\n",
        "      y_i_w = counter_i.get(word, 0) # get word frequency from i corpus\n",
        "      y_j_w = counter_j.get(word, 0) # get word frequency from j corpus\n",
        "\n",
        "      # Compute log-odds\n",
        "      log_odds = math.log((y_i_w + a_w)/(n_i + a_0 - y_i_w - a_w)) - math.log((y_j_w + a_w)/(n_j + a_0 - y_j_w - a_w))\n",
        "\n",
        "      # Compute variance\n",
        "      variance = (1/(y_i_w + a_w)) + (1/(y_j_w + a_w))\n",
        "\n",
        "      # Compute Dirichlet prior\n",
        "      dirichlet_prior = log_odds / math.sqrt(variance)\n",
        "      z_scores.add(dirichlet_prior)\n",
        "\n",
        "\n",
        "logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddu4uK9pE9T0"
      },
      "outputs": [],
      "source": [
        "logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxGYvttDv2Pc"
      },
      "source": [
        "To check your work, you can run log-odds on the party platforms from the lab section. With `nltk.word_tokenize` _before_ lower-casing, these should be your top 5 words (and scores, roughly). Depending on your tokenization strategy, your scores might be slightly different.\n",
        "\n",
        "**Democrat**:\n",
        "```\n",
        "president:\t4.75\n",
        "biden:\t4.27\n",
        "to:\t4.11\n",
        "he:\t4.09\n",
        "has:\t4.08\n",
        "```\n",
        "**Republican**\n",
        "```\n",
        "republicans:\t-13.45\n",
        "our:\t-11.23\n",
        "will:\t-10.88\n",
        "american:\t-10.01\n",
        "restore:\t-7.97\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDnm5YrXv2Pc"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I10MrjXyv2Pd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "logodds_with_uninformative_prior(\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_democrat_party_platform.txt\")],\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_republican_party_platform.txt\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXhC_JlIv2Pd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}