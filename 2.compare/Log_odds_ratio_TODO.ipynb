{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoldPapaya/info256-applied-nlp/blob/main/2.compare/Log_odds_ratio_TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGOJIv5Xv2PR"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/2.compare/Log_odds_ratio_TODO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdY9gXc3v2PS"
      },
      "source": [
        "# Log odds-ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVpdSX0E9Tw"
      },
      "source": [
        "The log odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
        "\n",
        "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
        "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
        "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
        "\n",
        "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sezbdfE9Ty"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset.\n",
        "   \n",
        "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\"\n",
        "\n",
        "**Describe each of those datasets and their source in 100-200 words.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtwPan7Kv2PV"
      },
      "source": [
        "I selected to use the transcript of the movie \"Oppenheimer\" (dataset 1) and the movie \"Barbie\" (dataset 2). I thought it might be interesting to see if any obvious themes between the two movies might be visible through the type of language they were using. Comparisons between the two films were quite popular when they launched, as they were both released around the same time and had almost polar opposite plots. I picked [scrapsfromtheloft.com](https://scrapsfromtheloft.com/), a website that features transcripts of various movies, to acquire data. To do this, I manually copy pasted the transcripts of both \"Oppenheimer\" and \"Barbie\" into their own respective files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4_1a4EE9Ty"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure dependencies are installed\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWfjo_IYxZ4n",
        "outputId": "9633c73e-41b5-46a9-ba8b-e30f569989d5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6eYQtYiCE9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30fc4eb-59bc-4634-ce35-e148d73ee0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk # using nltk for tokenization\n",
        "nltk.download('punkt_tab')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def read_and_tokenize(filename: str) -> list[str]:\n",
        "    document = open(filename).read()\n",
        "    return word_tokenize(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "z28xc1Y1E9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4de5a2-ed89-45ed-bfd5-beebf83bac88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-05 20:30:48--  https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class1_dataset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112470 (110K) [text/plain]\n",
            "Saving to: ‘class1_dataset.txt.5’\n",
            "\n",
            "\rclass1_dataset.txt.   0%[                    ]       0  --.-KB/s               \rclass1_dataset.txt. 100%[===================>] 109.83K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-05 20:30:48 (4.34 MB/s) - ‘class1_dataset.txt.5’ saved [112470/112470]\n",
            "\n",
            "--2025-09-05 20:30:49--  https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class2_dataset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86591 (85K) [text/plain]\n",
            "Saving to: ‘class2_dataset.txt.5’\n",
            "\n",
            "class2_dataset.txt. 100%[===================>]  84.56K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-05 20:30:49 (3.61 MB/s) - ‘class2_dataset.txt.5’ saved [86591/86591]\n",
            "\n",
            "['CAPTION', ':', '“', 'PROMETHEUS', 'STOLE', 'FIRE', 'FROM', 'THE', 'GODS', 'AND'] 25036 3431\n",
            "['[', 'narrator', ']', 'Since', 'the', 'beginning', 'of', 'time', ',', 'since'] 21395 2509\n"
          ]
        }
      ],
      "source": [
        "# change these file paths to wherever the datasets you created above live.\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class1_dataset.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/GoldPapaya/info256-applied-nlp/main/data/class2_dataset.txt\n",
        "\n",
        "class1_tokens = read_and_tokenize(\"class1_dataset.txt\")\n",
        "class2_tokens = read_and_tokenize(\"class2_dataset.txt\")\n",
        "\n",
        "print(class1_tokens[:10], len(class1_tokens), len(set(class1_tokens))) # Oppenheimer print output: sample set of tokens, # of tokens, # of unique tokens\n",
        "print(class2_tokens[:10], len(class2_tokens), len(set(class2_tokens))) # Barbie print output: sample set of tokens, # of tokens, # of unique tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHj4k4tE9Tz"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior. This value, $\\widehat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\widehat{\\zeta}_w^{(i-j)}= {\\widehat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\widehat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
        "* $\\alpha_w$ = 0.01\n",
        "* $V$ = size of vocabulary (number of distinct word types)\n",
        "* $\\alpha_0 = V * \\alpha_w$\n",
        "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
        "\n",
        "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "lHWahiy8E9T0"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def logodds_with_uninformative_prior(tokens_i: list[str], tokens_j: list[str], display=25):\n",
        "    n_i = len(tokens_i) # number of tokens in i\n",
        "    n_j = len(tokens_j) # number of tokens in j\n",
        "    counter_i = Counter(tokens_i) # freq dict of i\n",
        "    counter_j = Counter(tokens_j) # freq dict of j\n",
        "    vocabulary = set(tokens_i).union(set(tokens_j)) # union of all types between i and j\n",
        "    V = len(vocabulary) # size of vocab\n",
        "    a_w = 0.01\n",
        "    a_0 = V * a_w\n",
        "\n",
        "    z_scores = {}\n",
        "    for word in vocabulary:\n",
        "      y_i_w = counter_i.get(word, 0) # get word frequency from i corpus\n",
        "      y_j_w = counter_j.get(word, 0) # get word frequency from j corpus\n",
        "\n",
        "      # Compute log-odds\n",
        "      log_odds = math.log((y_i_w + a_w)/(n_i + a_0 - y_i_w - a_w)) - math.log((y_j_w + a_w)/(n_j + a_0 - y_j_w - a_w))\n",
        "\n",
        "      # Compute variance\n",
        "      variance = (1/(y_i_w + a_w)) + (1/(y_j_w + a_w))\n",
        "\n",
        "      # Compute z-score\n",
        "      z = log_odds / math.sqrt(variance)\n",
        "      z_scores[word] = z\n",
        "\n",
        "    sorted_tokens = sorted(z_scores.items(), key=lambda x: x[1], reverse=True) # sort words by their respective z score\n",
        "\n",
        "    print(\"Top 25 words for class1_dataset\")\n",
        "    count = 0\n",
        "    for word, z in sorted_tokens:\n",
        "        if z > 0:\n",
        "            print(f\"{word}: {z:.2f}\")\n",
        "            count += 1\n",
        "            if count >= 25:\n",
        "                break\n",
        "\n",
        "    print(\"\\nTop 25 words for class2_dataset\")\n",
        "    count = 0\n",
        "    for word, z in sorted_tokens[::-1]:  # Reverse for negative z-scores\n",
        "        if z < 0:\n",
        "            print(f\"{word}: {z:.2f}\")\n",
        "            count += 1\n",
        "            if count >= 25:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Ddu4uK9pE9T0",
        "outputId": "83177dcf-04d5-42f6-8315-16d2dffe7dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words for class1_dataset\n",
            "the: 10.06\n",
            "of: 6.77\n",
            "he: 6.29\n",
            "?: 5.96\n",
            "to: 5.59\n",
            "was: 5.39\n",
            "him: 4.97\n",
            "they: 4.64\n",
            "would: 4.58\n",
            "Well: 4.50\n",
            "in: 4.01\n",
            "a: 3.99\n",
            "Robert: 3.95\n",
            "He: 3.91\n",
            "his: 3.86\n",
            "not: 3.71\n",
            ",: 3.62\n",
            "as: 3.60\n",
            "A: 3.53\n",
            "security: 3.44\n",
            "d: 3.43\n",
            "did: 3.43\n",
            "Los: 3.42\n",
            "from: 3.39\n",
            "years: 3.31\n",
            "\n",
            "Top 25 words for class2_dataset\n",
            "!: -10.49\n",
            "]: -8.47\n",
            "[: -8.47\n",
            "Oh: -7.29\n",
            "Okay: -5.40\n",
            "Yeah: -5.29\n",
            "Hi: -5.24\n",
            "so: -5.14\n",
            "just: -5.11\n",
            "na: -4.66\n",
            "I: -4.65\n",
            "her: -4.45\n",
            "m: -4.21\n",
            "music: -4.18\n",
            "playing: -4.12\n",
            "night: -4.12\n",
            "And: -3.98\n",
            "love: -3.96\n",
            "like: -3.78\n",
            "go: -3.57\n",
            "She: -3.54\n",
            "World: -3.46\n",
            "got: -3.43\n",
            "she: -3.40\n",
            "Hey: -3.40\n"
          ]
        }
      ],
      "source": [
        "logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxGYvttDv2Pc"
      },
      "source": [
        "To check your work, you can run log-odds on the party platforms from the lab section. With `nltk.word_tokenize` _before_ lower-casing, these should be your top 5 words (and scores, roughly). Depending on your tokenization strategy, your scores might be slightly different.\n",
        "\n",
        "**Democrat**:\n",
        "```\n",
        "president:\t4.75\n",
        "biden:\t4.27\n",
        "to:\t4.11\n",
        "he:\t4.09\n",
        "has:\t4.08\n",
        "```\n",
        "**Republican**\n",
        "```\n",
        "republicans:\t-13.45\n",
        "our:\t-11.23\n",
        "will:\t-10.88\n",
        "american:\t-10.01\n",
        "restore:\t-7.97\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FDnm5YrXv2Pc",
        "outputId": "61f04845-fb85-40a2-a9e2-d30c35dc24f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-05 20:30:50--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283046 (276K) [text/plain]\n",
            "Saving to: ‘2024_democrat_party_platform.txt.5’\n",
            "\n",
            "2024_democrat_party 100%[===================>] 276.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-05 20:30:50 (7.61 MB/s) - ‘2024_democrat_party_platform.txt.5’ saved [283046/283046]\n",
            "\n",
            "--2025-09-05 20:30:50--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35319 (34K) [text/plain]\n",
            "Saving to: ‘2024_republican_party_platform.txt.5’\n",
            "\n",
            "2024_republican_par 100%[===================>]  34.49K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-09-05 20:30:50 (3.71 MB/s) - ‘2024_republican_party_platform.txt.5’ saved [35319/35319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "I10MrjXyv2Pd",
        "outputId": "404c96b9-9d63-4a15-82c9-2682a303b4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words for class1_dataset\n",
            "president: 4.75\n",
            "biden: 4.27\n",
            "to: 4.11\n",
            "he: 4.09\n",
            "has: 4.08\n",
            "more: 3.76\n",
            "democrats: 3.39\n",
            "for: 3.14\n",
            "also: 3.13\n",
            "administration: 3.06\n",
            "'s: 3.06\n",
            "his: 2.93\n",
            "a: 2.90\n",
            "is: 2.82\n",
            "$: 2.77\n",
            "in: 2.56\n",
            "than: 2.51\n",
            "care: 2.51\n",
            "communities: 2.49\n",
            "as: 2.30\n",
            "continue: 2.28\n",
            "working: 2.26\n",
            "work: 2.26\n",
            "americans: 2.25\n",
            "year: 2.23\n",
            "\n",
            "Top 25 words for class2_dataset\n",
            "republicans: -13.45\n",
            "our: -11.23\n",
            "will: -10.88\n",
            "american: -10.01\n",
            "restore: -7.97\n",
            "great: -7.11\n",
            "illegal: -6.32\n",
            "republican: -6.07\n",
            "policies: -5.91\n",
            ":: -5.80\n",
            "stop: -5.79\n",
            "again: -5.56\n",
            "we: -5.55\n",
            "inflation: -5.55\n",
            "4: -5.51\n",
            "must: -5.44\n",
            "1: -5.32\n",
            "party: -5.30\n",
            "3: -5.28\n",
            "common: -5.18\n",
            "bring: -5.15\n",
            "peace: -5.14\n",
            "5: -5.12\n",
            "education: -5.11\n",
            "commitment: -4.94\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "logodds_with_uninformative_prior(\n",
        "    [w.lower() for w in nltk.word_tokenize(open(\"2024_democrat_party_platform.txt\").read())],\n",
        "    [w.lower() for w in nltk.word_tokenize(open(\"2024_republican_party_platform.txt\").read())]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "PXhC_JlIv2Pd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}